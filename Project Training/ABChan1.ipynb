{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import folium\n",
    "from sklearn.cluster import KMeans\n",
    "from shapely.geometry import Point\n",
    "from scipy.spatial import ConvexHull\n",
    "from itertools import product\n",
    "from datetime import timedelta\n",
    "\n",
    "# Load the dataset\n",
    "traffic_df = pd.read_csv('Traffic_Incidents_20241114.csv')  # Adjust this path if necessary\n",
    "# Use KMeans clustering to \n",
    "n_clusters = 6\n",
    "# set these according to your actual data\n",
    "n_history_days = 14\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:21: SyntaxWarning: invalid escape sequence '\\T'\n",
      "<>:21: SyntaxWarning: invalid escape sequence '\\T'\n",
      "C:\\Users\\zohre\\AppData\\Local\\Temp\\ipykernel_18944\\2933940307.py:21: SyntaxWarning: invalid escape sequence '\\T'\n",
      "  print(f\"\\Traffic Incidents Data Covers from {start_date} to {end_date}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\Traffic Incidents Data Covers from 2016-12-06 to 2024-11-14\n",
      "Missing Dates or no accidence date List (first 10):\n",
      " [datetime.date(2016, 12, 21), datetime.date(2016, 12, 22), datetime.date(2016, 12, 23), datetime.date(2016, 12, 24), datetime.date(2016, 12, 25), datetime.date(2016, 12, 26), datetime.date(2016, 12, 27), datetime.date(2016, 12, 28), datetime.date(2016, 12, 29), datetime.date(2016, 12, 30)]\n",
      "Recorded Days: 2744\n",
      "All Days: 2901\n",
      "Missing Days: 157\n",
      "\n",
      "Confirmed: No rows exist for the identified missing dates.\n",
      "\n",
      "Missing Values Summary:\n",
      "                      Column  Missing Values  Percentage Missing\n",
      "QUADRANT            QUADRANT           14059           27.693187\n",
      "MODIFIED_DT      MODIFIED_DT           14057           27.689247\n",
      "DESCRIPTION      DESCRIPTION               2            0.003940\n",
      "Date                    Date               0            0.000000\n",
      "Time                    Time               0            0.000000\n",
      "INCIDENT INFO  INCIDENT INFO               0            0.000000\n",
      "START_DT            START_DT               0            0.000000\n",
      "Longitude          Longitude               0            0.000000\n",
      "Latitude            Latitude               0            0.000000\n",
      "Count                  Count               0            0.000000\n",
      "id                        id               0            0.000000\n",
      "Point                  Point               0            0.000000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Ensure START_DT column is in datetime format\n",
    "traffic_df['START_DT'] = pd.to_datetime(traffic_df['START_DT'], errors='coerce')  # Handle potential parsing errors\n",
    "\n",
    "# Drop rows where START_DT could not be converted to datetime (if any)\n",
    "#traffic_df = traffic_df.dropna(subset=['START_DT'])\n",
    "\n",
    "# Create separate Date and Time columns\n",
    "traffic_df['Date'] = traffic_df['START_DT'].dt.date  # Extract the date\n",
    "traffic_df['Time'] = traffic_df['START_DT'].dt.strftime('%H:%M:%S')  # Extract the time in 24-hour format as string\n",
    "traffic_df['Date'] = pd.to_datetime(traffic_df['Date'])\n",
    "# Sort by START_DT to ensure both date and time are in order\n",
    "traffic_df = traffic_df.sort_values(by='START_DT', ascending=True)\n",
    "\n",
    "# Rearrange columns to place Date and Time after START_DT (optional)\n",
    "columns_order = ['Date', 'Time'] + [col for col in traffic_df.columns if col not in ['Date', 'Time']]\n",
    "traffic_df = traffic_df[columns_order]\n",
    "\n",
    "# Determine the start and end START_DT of the filtered data\n",
    "start_date = traffic_df['Date'].min().date()\n",
    "end_date = traffic_df['Date'].max().date()\n",
    "print(f\"\\Traffic Incidents Data Covers from {start_date} to {end_date}\")\n",
    "\n",
    "# Generate a complete date range\n",
    "all_dates = pd.date_range(start=start_date, end=end_date).date\n",
    "# Identify recorded dates in the dataset\n",
    "recorded_dates = set(traffic_df['Date'].dt.date)\n",
    "# Identify missing dates\n",
    "missing_dates = sorted(set(all_dates) - recorded_dates)\n",
    "\n",
    "if len(missing_dates) > 0:\n",
    "    print(f\"Missing Dates or no accidence date List (first 10):\\n {missing_dates[:10]}\")\n",
    "\n",
    "# Count missing and recorded dates\n",
    "print(f\"Recorded Days: {len(recorded_dates)}\")\n",
    "print(f\"All Days: {len(all_dates)}\")\n",
    "print(f\"Missing Days: {len(missing_dates)}\")\n",
    "\n",
    "# Verify that no rows exist for the identified missing dates\n",
    "missing_rows = traffic_df[traffic_df['Date'].dt.date.isin(missing_dates)]\n",
    "\n",
    "if not missing_rows.empty:\n",
    "    print(\"\\nUnexpectedly Found Rows for Missing Dates:\")\n",
    "    print(missing_rows)\n",
    "else:\n",
    "    print(\"\\nConfirmed: No rows exist for the identified missing dates.\")\n",
    "\n",
    "# Check for missing values in each column  \n",
    "missing_values = traffic_df.isnull().sum()\n",
    "total_rows = len(traffic_df)\n",
    "\n",
    "# Create a DataFrame for a clear overview\n",
    "missing_summary = pd.DataFrame({\n",
    "    'Column': traffic_df.columns,\n",
    "    'Missing Values': missing_values,\n",
    "    'Percentage Missing': (missing_values / total_rows) * 100\n",
    "})\n",
    "\n",
    "# Display missing summary\n",
    "missing_summary.sort_values(by='Percentage Missing', ascending=False, inplace=True)\n",
    "print(\"\\nMissing Values Summary:\")\n",
    "print(missing_summary)\n",
    "    \n",
    "# Save the modified DataFrame to a CSV file\n",
    "# traffic_df.to_csv(\"justToCheck1.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Map saved as 'Traffic_Map_Detailed.html'\n"
     ]
    }
   ],
   "source": [
    "kmeans = KMeans(n_clusters, random_state=0)\n",
    "traffic_df['Cluster_KMeans'] = kmeans.fit_predict(traffic_df[['Latitude', 'Longitude']])\n",
    "\n",
    "# Create a map to visualize clusters\n",
    "map_houston = folium.Map(location=[traffic_df['Latitude'].mean(), traffic_df['Longitude'].mean()], zoom_start=11)\n",
    "colors = ['#3186cc', '#FF5733', '#33FF57', '#f1c40f', '#8e44ad', '#3498db', '#e74c3c']\n",
    "color_dict = {k: colors[k % len(colors)] for k in range(n_clusters)}\n",
    "\n",
    "for cluster_id in range(n_clusters):\n",
    "    cluster_points = traffic_df[traffic_df['Cluster_KMeans'] == cluster_id]\n",
    "    points = cluster_points[['Longitude', 'Latitude']].values\n",
    "\n",
    "    if len(points) > 2:\n",
    "        hull = ConvexHull(points)\n",
    "        hull_points = [(points[vertex][1], points[vertex][0]) for vertex in hull.vertices]\n",
    "        folium.Polygon(locations=hull_points,\n",
    "                       color=color_dict[cluster_id],\n",
    "                       fill=True,\n",
    "                       fill_color=color_dict[cluster_id],\n",
    "                       fill_opacity=0.5,\n",
    "                       tooltip=f'Cluster {cluster_id}, Points: {len(points)}').add_to(map_houston)\n",
    "\n",
    "    for _, row in cluster_points.iterrows():\n",
    "        folium.CircleMarker(location=(row['Latitude'], row['Longitude']),\n",
    "                            radius=2,\n",
    "                            color=color_dict[cluster_id],\n",
    "                            fill=True,\n",
    "                            fill_opacity=0.7).add_to(map_houston)\n",
    "\n",
    "map_houston.save(\"Traffic_Map_Detailed.html\")\n",
    "print(\"Map saved as 'Traffic_Map_Detailed.html'\")\n",
    "\n",
    "# Save the modified DataFrame to a CSV file\n",
    "# traffic_df.to_csv(\"justToCheck2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classified Incident Types and Their Counts:\n",
      "Incident_Type\n",
      "3    15645\n",
      "1    15096\n",
      "6     9328\n",
      "4     4915\n",
      "2     3917\n",
      "5     1866\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Function to classify descriptions into simplified categories\n",
    "def classify_description(description):\n",
    "    if isinstance(description, str):  # Check if the value is a string\n",
    "        description = description.lower()  # Convert to lowercase for consistency\n",
    "        if \"single vehicle\" in description:\n",
    "            return 2  # Single vehicle incident\n",
    "        elif \"two vehicle\" in description or \"2 vehicle\" in description:\n",
    "            return 3  # Two vehicle incident\n",
    "        elif \"multi-vehicle\" in description or \"multi vehicle\" in description:\n",
    "            return 4  # Multi-vehicle incident\n",
    "        elif \"pedestrian\" in description:\n",
    "            return 5  # Pedestrian involved\n",
    "        elif \"stalled\" in description or \"blocking\" in description:\n",
    "            return 6  # Stalled or obstructive\n",
    "        else:\n",
    "            return 1  # Other/Unknown\n",
    "    else:\n",
    "        return 1  # Default for non-string values\n",
    "\n",
    "# Apply classification function to the DESCRIPTION column\n",
    "traffic_df['Incident_Type'] = traffic_df['DESCRIPTION'].apply(classify_description)\n",
    "\n",
    "# Verify the classification\n",
    "incident_type_counts = traffic_df['Incident_Type'].value_counts()\n",
    "\n",
    "# Display the counts of each classified type\n",
    "print(\"\\nClassified Incident Types and Their Counts:\")\n",
    "print(incident_type_counts)\n",
    "\n",
    "# Save the modified DataFrame to a CSV file\n",
    "# traffic_df.to_csv(\"justToCheck3.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Modified DataFrame with 'Time_Period':\n",
      "                 START_DT  Time_Period\n",
      "6413  2016-12-06 10:00:00            0\n",
      "17172 2016-12-06 14:36:00            1\n",
      "8955  2016-12-06 16:25:00            1\n",
      "32623 2016-12-06 16:26:00            1\n",
      "32280 2016-12-06 17:05:00            1\n"
     ]
    }
   ],
   "source": [
    "# Define time period mapping function with numeric values\n",
    "def assign_time_period(hour):\n",
    "    if 6 <= hour < 12:\n",
    "        return 0  # Morning\n",
    "    elif 12 <= hour < 18:\n",
    "        return 1  # Lunch\n",
    "    elif 18 <= hour < 24:\n",
    "        return 2  # Evening\n",
    "    else:\n",
    "        return 3  # Midnight\n",
    "\n",
    "# Create the 'Time_Period' column with numeric codes\n",
    "traffic_df['Time_Period'] = pd.to_datetime(traffic_df['START_DT']).dt.hour.apply(assign_time_period)\n",
    "\n",
    "# Verify the modified DataFrame\n",
    "print(\"\\nModified DataFrame with 'Time_Period':\")\n",
    "print(traffic_df[['START_DT', 'Time_Period']].head())\n",
    "\n",
    "# Save the modified DataFrame to a CSV file\n",
    "# traffic_df.to_csv(\"justToCheck4.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traffic Incidents Data Covers from 2016-12-06 00:00:00 to 2024-11-14 00:00:00\n"
     ]
    }
   ],
   "source": [
    "# Define the start and end dates (shift start date backward by 14 days to include history)\n",
    "start_date = traffic_df['Date'].min()  # Shift start date backward by 14 days to include history\n",
    "end_date = traffic_df['Date'].max()\n",
    "print(f\"Traffic Incidents Data Covers from {start_date} to {end_date}\")\n",
    "\n",
    "# Generate all combinations of dates and time periods\n",
    "all_dates = pd.date_range(start=start_date, end=end_date)\n",
    "time_periods = [0, 1, 2, 3]  # Four time periods in a day\n",
    "\n",
    "# Create a DataFrame for all combinations of dates and time periods\n",
    "date_time_combinations = list(product(all_dates, time_periods))\n",
    "xydata = pd.DataFrame(date_time_combinations, columns=['Date', 'Time_Period'])\n",
    "\n",
    "# Determine the number of unique clusters\n",
    "n_clusters = traffic_df['Cluster_KMeans'].max() + 1\n",
    "\n",
    "# Add columns for clusters based on n_clusters value\n",
    "for cluster_num in range(n_clusters):\n",
    "    xydata[f'Cluster{cluster_num}'] = 0  # Initialize with 0 value for every cluster\n",
    "\n",
    "# Aggregate traffic_df to get counts of incidents per cluster\n",
    "traffic_counts = traffic_df.groupby(['Date', 'Time_Period', 'Cluster_KMeans']).size().reset_index(name='Count')\n",
    "\n",
    "# Update the counts in xydata DataFrame\n",
    "for cluster_num in range(n_clusters):\n",
    "    # Extract the data for the current cluster\n",
    "    cluster_data = traffic_counts[traffic_counts['Cluster_KMeans'] == cluster_num]\n",
    "    # Iterate through the rows and update the counts in xydata\n",
    "    for _, row in cluster_data.iterrows():\n",
    "        mask = (xydata['Date'] == row['Date']) & (xydata['Time_Period'] == row['Time_Period'])\n",
    "        xydata.loc[mask, f'Cluster{cluster_num}'] += int(row['Count'])\n",
    "\n",
    "# Export the result to CSV\n",
    "# xydata.to_csv(\"justToCheck5.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Date  Time_Period  Cluster0  Cluster1  Cluster2  Cluster3  Cluster4  \\\n",
      "0 2016-12-06            0         0         0         0         0         0   \n",
      "1 2016-12-06            1         0         1         0         1         0   \n",
      "2 2016-12-06            2         0         0         2         0         0   \n",
      "3 2016-12-06            3         0         0         0         0         0   \n",
      "4 2016-12-07            0         1         1         1         1         3   \n",
      "\n",
      "   Cluster5  C0D-1HA  C0D-2HA  ...  C5D-5HA  C5D-6HA  C5D-7HA  C5D-8HA  \\\n",
      "0         1        0        0  ...        0        0        0        0   \n",
      "1         4        0        0  ...        0        0        0        0   \n",
      "2         2        0        0  ...        0        0        0        0   \n",
      "3         0        0        0  ...        0        0        0        0   \n",
      "4         1        0        0  ...        0        0        0        0   \n",
      "\n",
      "   C5D-9HA  C5D-10HA  C5D-11HA  C5D-12HA  C5D-13HA  C5D-14HA  \n",
      "0        0         0         0         0         0         0  \n",
      "1        0         0         0         0         0         0  \n",
      "2        0         0         0         0         0         0  \n",
      "3        0         0         0         0         0         0  \n",
      "4        0         0         0         0         0         0  \n",
      "\n",
      "[5 rows x 92 columns]\n"
     ]
    }
   ],
   "source": [
    "# Generate new columns for each cluster for each history day\n",
    "for cluster_num in range(n_clusters):\n",
    "    for day in range(1, n_history_days + 1):\n",
    "        column_name = f'C{cluster_num}D-{day}HA'\n",
    "        xydata[column_name] = 0  # Initialize with 0 or another suitable value\n",
    "\n",
    "# Display the DataFrame to verify new columns\n",
    "print(xydata.head())\n",
    "\n",
    "# Export the result to CSV\n",
    "# xydata.to_csv(\"justToCheck6.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Date  Time_Period  Cluster0  Cluster1  Cluster2  Cluster3  Cluster4  \\\n",
      "0 2016-12-06            0         0         0         0         0         0   \n",
      "1 2016-12-06            1         0         1         0         1         0   \n",
      "2 2016-12-06            2         0         0         2         0         0   \n",
      "3 2016-12-06            3         0         0         0         0         0   \n",
      "4 2016-12-07            0         1         1         1         1         3   \n",
      "\n",
      "   Cluster5  C0D-1HA  C0D-2HA  ...  C5D-5HA  C5D-6HA  C5D-7HA  C5D-8HA  \\\n",
      "0         1      NaN      NaN  ...      NaN      NaN      NaN      NaN   \n",
      "1         4      NaN      NaN  ...      NaN      NaN      NaN      NaN   \n",
      "2         2      NaN      NaN  ...      NaN      NaN      NaN      NaN   \n",
      "3         0      NaN      NaN  ...      NaN      NaN      NaN      NaN   \n",
      "4         1      0.0      NaN  ...      NaN      NaN      NaN      NaN   \n",
      "\n",
      "   C5D-9HA  C5D-10HA  C5D-11HA  C5D-12HA  C5D-13HA  C5D-14HA  \n",
      "0      NaN       NaN       NaN       NaN       NaN       NaN  \n",
      "1      NaN       NaN       NaN       NaN       NaN       NaN  \n",
      "2      NaN       NaN       NaN       NaN       NaN       NaN  \n",
      "3      NaN       NaN       NaN       NaN       NaN       NaN  \n",
      "4      NaN       NaN       NaN       NaN       NaN       NaN  \n",
      "\n",
      "[5 rows x 92 columns]\n"
     ]
    }
   ],
   "source": [
    "# Ensure the DataFrame is sorted by Date for correct shifting\n",
    "xydata.sort_values(by=['Date', 'Time_Period'], inplace=True)\n",
    "\n",
    "# Generate new columns for each cluster for each history day and fill with shifted data\n",
    "for cluster_num in range(n_clusters):\n",
    "    for day in range(1, n_history_days + 1):\n",
    "        column_name = f'C{cluster_num}D-{day}HA'\n",
    "        # Shift data within each time period group\n",
    "        xydata[column_name] = xydata.groupby('Time_Period')[f'Cluster{cluster_num}'].shift(day)\n",
    "\n",
    "# Display the DataFrame to verify new columns\n",
    "print(xydata.head())\n",
    "\n",
    "# Optional: Export the result to CSV\n",
    "# xydata.to_csv(\"justToCheck7.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Date  Time_Period  Cluster0  Cluster1  Cluster2  Cluster3  Cluster4  \\\n",
      "0 2016-12-06            0         0         0         0         0         0   \n",
      "1 2016-12-06            1         0         1         0         1         0   \n",
      "2 2016-12-06            2         0         0         1         0         0   \n",
      "3 2016-12-06            3         0         0         0         0         0   \n",
      "4 2016-12-07            0         1         1         1         1         1   \n",
      "\n",
      "   Cluster5  C0D-1HA  C0D-2HA  ...  C5D-5HA  C5D-6HA  C5D-7HA  C5D-8HA  \\\n",
      "0         1      NaN      NaN  ...      NaN      NaN      NaN      NaN   \n",
      "1         1      NaN      NaN  ...      NaN      NaN      NaN      NaN   \n",
      "2         1      NaN      NaN  ...      NaN      NaN      NaN      NaN   \n",
      "3         0      NaN      NaN  ...      NaN      NaN      NaN      NaN   \n",
      "4         1      0.0      NaN  ...      NaN      NaN      NaN      NaN   \n",
      "\n",
      "   C5D-9HA  C5D-10HA  C5D-11HA  C5D-12HA  C5D-13HA  C5D-14HA  \n",
      "0      NaN       NaN       NaN       NaN       NaN       NaN  \n",
      "1      NaN       NaN       NaN       NaN       NaN       NaN  \n",
      "2      NaN       NaN       NaN       NaN       NaN       NaN  \n",
      "3      NaN       NaN       NaN       NaN       NaN       NaN  \n",
      "4      NaN       NaN       NaN       NaN       NaN       NaN  \n",
      "\n",
      "[5 rows x 92 columns]\n"
     ]
    }
   ],
   "source": [
    "# List of cluster columns to transform\n",
    "cluster_columns = [f'Cluster{i}' for i in range(cluster_num+1)]  \n",
    "# Apply transformation to each cluster column\n",
    "for column in cluster_columns:\n",
    "    xydata[column] = xydata[column].apply(lambda x: 1 if x > 0 else 0)\n",
    "\n",
    "# Display the DataFrame to verify the changes\n",
    "print(xydata.head())\n",
    "\n",
    "# Optional: Export the result to CSV\n",
    "xydata.to_csv(\"xydata_trafic.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
