{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd          # For data manipulation and handling\n",
    "import numpy as np           # For numerical operations\n",
    "from sklearn.model_selection import train_test_split  # For splitting the data into training and testing sets\n",
    "from sklearn.ensemble import RandomForestClassifier   # For the machine learning model\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix  # For evaluating the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "data = pd.read_csv('US_Accidents_March23_sampled_500k.csv')\n",
    "\n",
    "# Filter the data for rows where the City is \"Los Angeles\"\n",
    "data = data[data['City'] == 'Los Angeles']\n",
    "\n",
    "# Display the first few rows of the filtered data\n",
    "# print(la_data.head())\n",
    "# Display only the \"City\" column for rows where the City is \"Los Angeles\"\n",
    "# print(la_data['City'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the specified columns\n",
    "columns_to_drop = ['Description', 'End_Lng', 'End_Lat', 'Astronomical_Twilight', \n",
    "                   'Nautical_Twilight', 'Civil_Twilight', 'Country', 'Timezone', \n",
    "                   'Airport_Code', 'County', 'State', 'City']\n",
    "data = data.drop(columns=columns_to_drop)\n",
    "\n",
    "# Verify the changes\n",
    "print(data.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the 'Start_Time' column to datetime, setting invalid parsing as NaT\n",
    "data['Start_Time'] = pd.to_datetime(data['Start_Time'], errors='coerce', format='%Y-%m-%d %H:%M')\n",
    "\n",
    "# Drop rows where 'Start_Time' is NaT (invalid date format)\n",
    "data = data.dropna(subset=['Start_Time'])\n",
    "\n",
    "# Verify the changes by displaying the first few rows\n",
    "print(data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the number of rows in the data\n",
    "num_rows = data.shape[0]\n",
    "print(\"Number of rows:\", num_rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the number of missing values in each column before filling or dropping\n",
    "print(\"Missing values in each column before filling:\")\n",
    "print(data.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the specified columns\n",
    "columns_to_drop = ['Precipitation(in)', 'Wind_Chill(F)', 'Wind_Direction', 'Wind_Speed(mph)']\n",
    "data = data.drop(columns=columns_to_drop)\n",
    "\n",
    "# Verify the changes\n",
    "# print(data.head())\n",
    "print(data.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decide to remove some columns again\n",
    "columns_to_drop = ['ID', 'End_Time', 'Distance(mi)', 'Street', 'Zipcode', 'Weather_Timestamp','Pressure(in)','Source']\n",
    "data = data.drop(columns=columns_to_drop)\n",
    "\n",
    "# Verify the changes\n",
    "# print(data.head())\n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the columns to replace True/False with 1/0\n",
    "boolean_columns = [\n",
    "    'Amenity', 'Bump', 'Crossing', 'Give_Way', 'Junction', \n",
    "    'No_Exit', 'Railway', 'Roundabout', 'Station', 'Stop', \n",
    "    'Traffic_Calming', 'Traffic_Signal', 'Turning_Loop'\n",
    "]\n",
    "\n",
    "# Replace True/False with 1/0 for the specified columns\n",
    "data[boolean_columns] = data[boolean_columns].astype(int)\n",
    "\n",
    "# Verify the changes\n",
    "print(data[boolean_columns].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode 'Sunrise_Sunset' column: 1 for 'Day', 0 for 'Night'\n",
    "data['Sunrise_Sunset'] = data['Sunrise_Sunset'].map({'Day': 1, 'Night': 0})\n",
    "\n",
    "# Verify the changes\n",
    "print(data['Sunrise_Sunset'].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the mapping for weather conditions\n",
    "weather_mapping = {\n",
    "    'Clear': 1, 'Fair': 1,\n",
    "    'Cloudy': 2, 'Partly Cloudy': 2, 'Mostly Cloudy': 2, 'Overcast': 2,\n",
    "    'Cloudy / Windy': 2, 'Mostly Cloudy / Windy': 2,\n",
    "    'Light Rain': 3, 'Rain': 3, 'Heavy Rain': 3, 'Rain / Windy': 3, 'Light Rain with Thunder': 3,\n",
    "    'Thunder': 4, 'T-Storm': 4, 'Heavy T-Storm': 4,\n",
    "    'Fog': 5, 'Patches of Fog': 5, 'Mist': 5, 'Haze': 5,\n",
    "    'Smoke': 0, 'Scattered Clouds': 0, None: 0, '': 0  # Handle blanks and unknowns\n",
    "}\n",
    "\n",
    "# Apply the mapping to the 'Weather_Condition' column\n",
    "data['Weather_Condition'] = data['Weather_Condition'].map(weather_mapping)\n",
    "\n",
    "# Verify the changes\n",
    "print(data['Weather_Condition'].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the number of missing values in each column before filling or dropping\n",
    "print(\"Missing values in each column before filling:\")\n",
    "print(data.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows with missing values\n",
    "data = data.dropna()\n",
    "\n",
    "# Verify the changes\n",
    "print(\"Number of missing values in each column after removing rows:\")\n",
    "print(data.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the number of rows in the data\n",
    "num_rows = data.shape[0]\n",
    "print(\"Number of rows:\", num_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the \"Start_Time\" column is in datetime format\n",
    "data['Start_Time'] = pd.to_datetime(data['Start_Time'], errors='coerce')\n",
    "\n",
    "# Sort the data by \"Start_Time\" in ascending order (oldest to newest)\n",
    "data = data.sort_values(by='Start_Time', ascending=True).reset_index(drop=True)\n",
    "\n",
    "# Display the first few rows to verify the sorting\n",
    "print(data.head())\n",
    "# Save the data into an Excel file named \"x_features.xlsx\"\n",
    "output_file = \"x_features.xlsx\"\n",
    "data.to_excel(output_file, index=False)\n",
    "\n",
    "print(f\"Data has been saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.spatial import ConvexHull\n",
    "import folium\n",
    "# Step 1: Clustering the points\n",
    "coords = data[['Start_Lat', 'Start_Lng']].values\n",
    "n_clusters = 10  # Adjust the number of clusters\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "data['Cluster'] = kmeans.fit_predict(coords)\n",
    "\n",
    "# Step 2: Create Convex Hulls for each cluster\n",
    "cluster_polygons = []\n",
    "for cluster in range(n_clusters):\n",
    "    cluster_points = coords[data['Cluster'] == cluster]\n",
    "    if len(cluster_points) >= 3:  # ConvexHull requires at least 3 points\n",
    "        hull = ConvexHull(cluster_points)\n",
    "        polygon = [cluster_points[vertex] for vertex in hull.vertices]\n",
    "        cluster_polygons.append((cluster, polygon))\n",
    "\n",
    "# Step 3: Visualize using Folium\n",
    "map_la = folium.Map(location=[34.05, -118.25], zoom_start=10)\n",
    "\n",
    "# Add polygons to the map\n",
    "for cluster, polygon in cluster_polygons:\n",
    "    folium.Polygon(\n",
    "        locations=polygon,\n",
    "        color=f\"#{(cluster * 10000):06x}\",\n",
    "        fill=True,\n",
    "        fill_opacity=0.4,\n",
    "        popup=f\"Cluster {cluster}\"\n",
    "    ).add_to(map_la)\n",
    "\n",
    "# Add points to the map\n",
    "for _, row in data.iterrows():\n",
    "    folium.CircleMarker(\n",
    "        location=[row['Start_Lat'], row['Start_Lng']],\n",
    "        radius=2,\n",
    "        color='blue',\n",
    "        fill=True,\n",
    "        fill_opacity=0.6\n",
    "    ).add_to(map_la)\n",
    "\n",
    "# Save the map to an HTML file\n",
    "map_la.save(\"la_regions_map.html\")\n",
    "print(\"Map with regions saved as 'la_regions_map.html'. Open this file in a browser.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of clusters\n",
    "n_clusters = data['Cluster'].nunique()\n",
    "print(f\"Number of clusters: {n_clusters}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Initialize an empty DataFrame to store the output\n",
    "all_years_output = pd.DataFrame()\n",
    "\n",
    "# Define start date and end date\n",
    "start_date = datetime(2016, 3, 22)\n",
    "end_date = datetime(2023, 3, 31)\n",
    "\n",
    "# Iterate through all dates in the specified range\n",
    "current_date = start_date\n",
    "while current_date <= end_date:\n",
    "    # Filter data for the specific date\n",
    "    filtered_data = data[data['Start_Time'].dt.date == current_date.date()]\n",
    "    \n",
    "    # Count the total number of accidents (rows) for that date\n",
    "    total_accidents_for_day = len(filtered_data)\n",
    "    \n",
    "    # Count the number of accidents in each cluster\n",
    "    accidents_per_cluster = filtered_data['Cluster'].value_counts()\n",
    "    \n",
    "    # Include clusters with 0 accidents\n",
    "    all_clusters = range(10)  # Assuming 10 clusters\n",
    "    accidents_per_cluster = accidents_per_cluster.reindex(all_clusters, fill_value=0)\n",
    "    \n",
    "    # Calculate the total number of accidents on that date\n",
    "    total_accidents = accidents_per_cluster.sum()\n",
    "    \n",
    "    # Calculate the probability of accidents for each cluster\n",
    "    probability_per_cluster = accidents_per_cluster / total_accidents if total_accidents > 0 else accidents_per_cluster\n",
    "    \n",
    "    # Prepare the row for this specific date\n",
    "    row = {'Date': current_date.date()}\n",
    "    for cluster in all_clusters:\n",
    "        row[f'Cluster_{cluster}'] = probability_per_cluster[cluster]\n",
    "    row['Total_Accidents'] = total_accidents_for_day\n",
    "    \n",
    "    # Append the row to the output DataFrame\n",
    "    all_years_output = pd.concat([all_years_output, pd.DataFrame([row])], ignore_index=True)\n",
    "    \n",
    "    # Increment the date by one day\n",
    "    current_date += timedelta(days=1)\n",
    "\n",
    "# Save the output to an Excel file\n",
    "output_file = \"accident_data_2016_2023_subset.xlsx\"\n",
    "all_years_output.to_excel(output_file, index=False)\n",
    "\n",
    "print(f\"Accident data from {start_date.date()} to {end_date.date()} has been saved as '{output_file}'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the data for the specific date\n",
    "specific_date = \"2016-03-23\"\n",
    "filtered_data = data[data['Start_Time'].dt.date == pd.to_datetime(specific_date).date()]\n",
    "\n",
    "# Display the filtered data with the specified columns\n",
    "columns_to_display = [\n",
    "    'Severity', 'Start_Time', 'Start_Lat', 'Start_Lng', 'Temperature(F)',\n",
    "    'Humidity(%)', 'Visibility(mi)', 'Weather_Condition', 'Amenity', 'Bump',\n",
    "    'Crossing', 'Give_Way', 'Junction', 'No_Exit', 'Railway', 'Roundabout',\n",
    "    'Station', 'Stop', 'Traffic_Calming', 'Traffic_Signal', 'Turning_Loop',\n",
    "    'Sunrise_Sunset'\n",
    "]\n",
    "\n",
    "filtered_data = filtered_data[columns_to_display]\n",
    "\n",
    "# Display the data\n",
    "print(filtered_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Assuming 'data' is your DataFrame with binary columns and latitude/longitude\n",
    "binary_columns = [\n",
    "    \"Weather_Condition\", \"Amenity\", \"Bump\", \"Crossing\", \"Give_Way\", \n",
    "    \"Junction\", \"No_Exit\", \"Railway\", \"Roundabout\", \"Station\", \n",
    "    \"Stop\", \"Traffic_Calming\", \"Traffic_Signal\", \"Turning_Loop\", \n",
    "    \"Sunrise_Sunset\"\n",
    "]\n",
    "\n",
    "# Step 1: Perform clustering\n",
    "coords = data[['Start_Lat', 'Start_Lng']].values\n",
    "n_clusters = 10  # Adjust this if needed\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "data['Cluster'] = kmeans.fit_predict(coords)\n",
    "\n",
    "# Step 2: Create new cluster-specific columns\n",
    "for col in binary_columns:\n",
    "    for cluster in range(n_clusters):\n",
    "        new_col_name = f\"{col}_Cluster{cluster}\"\n",
    "        # Assign 1 to the respective cluster column if the cluster matches and the binary column value is 1\n",
    "        data[new_col_name] = np.where(data['Cluster'] == cluster, data[col], 0)\n",
    "\n",
    "# Drop the original 'Cluster' column if no longer needed\n",
    "# data = data.drop(columns=['Cluster'])\n",
    "\n",
    "# Save to Excel for review\n",
    "data.to_excel(\"x_features_clustered.xlsx\", index=False)\n",
    "\n",
    "print(\"New cluster-specific columns added and saved to 'x_features_clustered.xlsx'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Specific point to check\n",
    "# specific_point = np.array([[34.018902, -118.173264]])\n",
    "\n",
    "# # Predict the cluster for the specific point\n",
    "# predicted_cluster = kmeans.predict(specific_point)[0]\n",
    "\n",
    "# print(f\"The point {specific_point[0]} belongs to Cluster {predicted_cluster}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Load the data from the provided file\n",
    "file_name = \"x_features_clustered.xlsx\"\n",
    "data = pd.read_excel(file_name)\n",
    "\n",
    "# Select only the desired columns\n",
    "columns_to_keep = ['Start_Time', 'Weather_Condition', 'Cluster']\n",
    "filtered_data = data[columns_to_keep]\n",
    "\n",
    "# Save the filtered data to a new file\n",
    "output_file_name = \"Weather_Condition.xlsx\"\n",
    "filtered_data.to_excel(output_file_name, index=False)\n",
    "\n",
    "# import ace_tools as tools; tools.display_dataframe_to_user(name=\"Filtered Weather Condition Data\", dataframe=filtered_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Load the Weather_Condition file\n",
    "weather_data = pd.read_excel(\"Weather_Condition.xlsx\")\n",
    "\n",
    "# Create new columns for each cluster\n",
    "n_clusters = 10  # Assuming 10 clusters\n",
    "for cluster in range(n_clusters):\n",
    "    cluster_col_name = f\"Weather_Condition_Cluster{cluster}\"\n",
    "    # Assign weather condition to the corresponding cluster column\n",
    "    weather_data[cluster_col_name] = weather_data.apply(\n",
    "        lambda row: row[\"Weather_Condition\"] if row[\"Cluster\"] == cluster else 0, axis=1\n",
    "    )\n",
    "\n",
    "# Save the updated data back to the same file\n",
    "weather_data.to_excel(\"Weather_Condition.xlsx\", index=False)\n",
    "\n",
    "# import ace_tools as tools; tools.display_dataframe_to_user(name=\"Weather Condition with Cluster Columns\", dataframe=weather_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Read the Excel file\n",
    "file_path = \"Weather_Condition.xlsx\"  # Replace with the correct file path\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "# Ensure 'Start_Time' is in datetime format\n",
    "df[\"Start_Time\"] = pd.to_datetime(df[\"Start_Time\"])\n",
    "\n",
    "# Extract the date from Start_Time to group by date\n",
    "df[\"Date\"] = df[\"Start_Time\"].dt.date\n",
    "\n",
    "# Group by Date and Cluster to count the number of points and aggregate weather conditions\n",
    "summary = (\n",
    "    df.groupby([\"Date\", \"Cluster\"])\n",
    "    .agg(\n",
    "        Points_Count=(\"Weather_Condition\", \"size\"),  # Count the number of points\n",
    "        Weather_Condition_Mode=(\"Weather_Condition\", lambda x: x.mode()[0] if not x.mode().empty else None),  # Calculate the mode\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Optionally, save the summary to a new Excel file\n",
    "summary.to_excel(\"Cluster_Weather_Condition_Summary.xlsx\", index=False)\n",
    "\n",
    "# Display the result\n",
    "# import ace_tools as tools; tools.display_dataframe_to_user(name=\"Cluster Weather Condition Summary\", dataframe=summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Load your summarized data\n",
    "summary_file = \"Cluster_Weather_Condition_Summary.xlsx\"  # Replace with your file path\n",
    "summary = pd.read_excel(summary_file)\n",
    "\n",
    "# Create a list of all unique cluster column names\n",
    "clusters = [f\"Weather_Condition_Cluster{i}\" for i in range(10)]\n",
    "\n",
    "# Initialize an empty DataFrame for the final result\n",
    "final_data = pd.DataFrame(columns=[\"Date\"] + clusters)\n",
    "\n",
    "# Iterate through each date\n",
    "for date, group in summary.groupby(\"Date\"):\n",
    "    # Create a row with all clusters initialized to 0\n",
    "    row = {cluster: 0 for cluster in clusters}\n",
    "    row[\"Date\"] = date\n",
    "    \n",
    "    # Populate the mode for each cluster\n",
    "    for _, row_data in group.iterrows():\n",
    "        cluster_name = f\"Weather_Condition_Cluster{int(row_data['Cluster'])}\"\n",
    "        row[cluster_name] = row_data[\"Weather_Condition_Mode\"]\n",
    "    \n",
    "    # Append the row to the final DataFrame\n",
    "    final_data = pd.concat([final_data, pd.DataFrame([row])], ignore_index=True)\n",
    "\n",
    "# Save the final DataFrame to a new Excel file\n",
    "final_data.to_excel(\"Aggregated_Weather_Conditions_By_Date.xlsx\", index=False)\n",
    "\n",
    "# Display the result\n",
    "# import ace_tools as tools; tools.display_dataframe_to_user(name=\"Aggregated Weather Conditions by Date\", dataframe=final_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Load the data from the provided file\n",
    "file_name = \"x_features_clustered.xlsx\"  \n",
    "data = pd.read_excel(file_name)\n",
    "\n",
    "# Select only the desired columns\n",
    "columns_to_keep = ['Start_Time', 'Severity', 'Cluster']  \n",
    "filtered_data = data[columns_to_keep]\n",
    "\n",
    "# Save the filtered data to a new file\n",
    "output_file_name = \"Severity_Condition.xlsx\"\n",
    "filtered_data.to_excel(output_file_name, index=False)\n",
    "\n",
    "# Ensure 'Start_Time' is in datetime format\n",
    "filtered_data[\"Start_Time\"] = pd.to_datetime(filtered_data[\"Start_Time\"])\n",
    "\n",
    "# Extract the date from Start_Time to group by date\n",
    "filtered_data[\"Date\"] = filtered_data[\"Start_Time\"].dt.date\n",
    "\n",
    "# Group by Date and Cluster to count the number of points and aggregate severity\n",
    "summary = (\n",
    "    filtered_data.groupby([\"Date\", \"Cluster\"])\n",
    "    .agg(\n",
    "        Points_Count=(\"Severity\", \"size\"),  # Count the number of points\n",
    "        Severity_Mode=(\"Severity\", lambda x: x.mode()[0] if not x.mode().empty else None),  # Calculate the mode of Severity\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Save the summary to a new Excel file\n",
    "summary_file = \"Cluster_Severity_Summary.xlsx\"\n",
    "summary.to_excel(summary_file, index=False)\n",
    "\n",
    "# Create a list of all unique cluster column names\n",
    "clusters = [f\"Severity_Cluster{i}\" for i in range(10)]\n",
    "\n",
    "# Initialize an empty DataFrame for the final result\n",
    "final_data = pd.DataFrame(columns=[\"Date\"] + clusters)\n",
    "\n",
    "# Iterate through each date\n",
    "for date, group in summary.groupby(\"Date\"):\n",
    "    # Create a row with all clusters initialized to 0\n",
    "    row = {cluster: 0 for cluster in clusters}\n",
    "    row[\"Date\"] = date\n",
    "    \n",
    "    # Populate the mode for each cluster\n",
    "    for _, row_data in group.iterrows():\n",
    "        cluster_name = f\"Severity_Cluster{int(row_data['Cluster'])}\"\n",
    "        row[cluster_name] = row_data[\"Severity_Mode\"]\n",
    "    \n",
    "    # Append the row to the final DataFrame\n",
    "    final_data = pd.concat([final_data, pd.DataFrame([row])], ignore_index=True)\n",
    "\n",
    "# Save the final DataFrame to a new Excel file\n",
    "final_data.to_excel(\"Aggregated_Severity_By_Date.xlsx\", index=False)\n",
    "\n",
    "# Display the result\n",
    "# import ace_tools as tools; tools.display_dataframe_to_user(name=\"Aggregated Severity by Date\", dataframe=final_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the Excel file\n",
    "file_name = \"x_features_clustered.xlsx\"  # Replace with the actual file path\n",
    "data = pd.read_excel(file_name)\n",
    "\n",
    "# Drop the \"Severity\" and \"Weather_Condition\" columns, and all \"Weather_Condition_ClusterX\" columns\n",
    "columns_to_drop = ['Severity', 'Weather_Condition','cluster'] + [f\"Weather_Condition_Cluster{i}\" for i in range(10)]\n",
    "data = data.drop(columns=columns_to_drop, errors='ignore')\n",
    "\n",
    "# Save the updated data to a new Excel file\n",
    "output_file_name = \"x_features_clustered_cleaned.xlsx\"\n",
    "data.to_excel(output_file_name, index=False)\n",
    "\n",
    "# Display the result\n",
    "# import ace_tools as tools; tools.display_dataframe_to_user(name=\"Cleaned Features Clustered Data\", dataframe=data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Load the data\n",
    "file_name = \"x_features_clustered_cleaned.xlsx\"  # Replace with the correct file name\n",
    "data = pd.read_excel(file_name)\n",
    "\n",
    "# Ensure the 'Start_Time' column is in datetime format\n",
    "data['Start_Time'] = pd.to_datetime(data['Start_Time'])\n",
    "\n",
    "# Define the start and end dates\n",
    "start_date = datetime(2016, 3, 22)\n",
    "end_date = datetime(2023, 3, 31)\n",
    "\n",
    "# Prepare a list to store aggregated rows\n",
    "aggregated_rows = []\n",
    "\n",
    "# Loop through each day in the range\n",
    "current_date = start_date\n",
    "while current_date <= end_date:\n",
    "    # Filter the data for the specific date\n",
    "    filtered_data = data[data['Start_Time'].dt.date == current_date.date()]\n",
    "    \n",
    "    # Skip if there's no data for the current date\n",
    "    if filtered_data.empty:\n",
    "        current_date += timedelta(days=1)\n",
    "        continue\n",
    "    \n",
    "    # Initialize aggregated data for the specific date\n",
    "    aggregated_row = {'Date': current_date.date()}\n",
    "    \n",
    "    # Step 1: Calculate mean for specific numeric columns\n",
    "    numeric_columns = ['Temperature(F)', 'Humidity(%)', 'Visibility(mi)']  # Replace with actual numeric columns\n",
    "    for col in numeric_columns:\n",
    "        aggregated_row[col] = filtered_data[col].mean()\n",
    "    \n",
    "    # Step 2: Process binary and categorical columns by cluster\n",
    "    binary_columns = [\n",
    "        'Amenity', 'Bump', 'Crossing', 'Give_Way', 'Junction', 'No_Exit', 'Railway',\n",
    "        'Roundabout', 'Station', 'Stop', 'Traffic_Calming', 'Traffic_Signal', \n",
    "        'Turning_Loop', 'Sunrise_Sunset'  # Replace with actual binary columns\n",
    "    ]\n",
    "    n_clusters = 10  # Assuming 10 clusters\n",
    "    for cluster in range(n_clusters):\n",
    "        cluster_data = filtered_data[filtered_data['Cluster'] == cluster]\n",
    "        \n",
    "        # Process binary columns\n",
    "        for col in binary_columns:\n",
    "            column_name = f\"{col}_Cluster{cluster}\"\n",
    "            if not cluster_data.empty:\n",
    "                # Calculate the mean for binary columns (probability)\n",
    "                aggregated_row[column_name] = cluster_data[col].mean()\n",
    "            else:\n",
    "                aggregated_row[column_name] = 0  # Default to 0 if no points in cluster\n",
    "    \n",
    "    # Append the aggregated row for the current date\n",
    "    aggregated_rows.append(aggregated_row)\n",
    "    \n",
    "    # Move to the next day\n",
    "    current_date += timedelta(days=1)\n",
    "\n",
    "# Convert all aggregated rows into a DataFrame\n",
    "aggregated_df = pd.DataFrame(aggregated_rows)\n",
    "\n",
    "# Save the result to an Excel file\n",
    "output_file_name = \"aggregated_data_2016_to_2023.xlsx\"\n",
    "aggregated_df.to_excel(output_file_name, index=False)\n",
    "\n",
    "print(f\"Aggregated data from 2016-03-22 to 2023-03-31 has been saved to '{output_file_name}'.\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Load the three files\n",
    "weather_file = \"Aggregated_Weather_Conditions_By_Date.xlsx\"\n",
    "severity_file = \"Aggregated_Severity_By_Date.xlsx\"\n",
    "aggregated_file = \"aggregated_data_2016_to_2023.xlsx\"\n",
    "\n",
    "weather_data = pd.read_excel(weather_file)\n",
    "severity_data = pd.read_excel(severity_file)\n",
    "aggregated_data = pd.read_excel(aggregated_file)\n",
    "\n",
    "# Drop the \"Date\" column from weather_data and severity_data\n",
    "weather_data = weather_data.drop(columns=['Date'], errors='ignore')\n",
    "severity_data = severity_data.drop(columns=['Date'], errors='ignore')\n",
    "\n",
    "# Combine the columns from weather_data and severity_data into aggregated_data\n",
    "final_data = pd.concat([aggregated_data, weather_data, severity_data], axis=1)\n",
    "\n",
    "# Save the final result to a new Excel file\n",
    "output_file_name = \"final_aggregated_data.xlsx\"\n",
    "final_data.to_excel(output_file_name, index=False)\n",
    "\n",
    "print(f\"The final aggregated data has been saved to '{output_file_name}'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Load the Excel file\n",
    "file_name = \"final_aggregated_data.xlsx\"  # Replace with the correct file name\n",
    "data = pd.read_excel(file_name)\n",
    "\n",
    "# Remove the time part from the Date column\n",
    "data[\"Date\"] = pd.to_datetime(data[\"Date\"]).dt.date\n",
    "\n",
    "# Save the updated data to a new file\n",
    "output_file_name = \"Final_X_Features.xlsx\"\n",
    "data.to_excel(output_file_name, index=False)\n",
    "\n",
    "print(f\"The updated file has been saved as '{output_file_name}'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# File names\n",
    "file1 = \"Final_X_Features.xlsx\"\n",
    "file2 = \"accident_data_2016_2023_subset.xlsx\"\n",
    "\n",
    "# Load the files\n",
    "data1 = pd.read_excel(file1)\n",
    "data2 = pd.read_excel(file2)\n",
    "\n",
    "# Get the number of rows for each file\n",
    "rows_file1 = len(data1)\n",
    "rows_file2 = len(data2)\n",
    "\n",
    "# Display the results\n",
    "print(f\"Number of rows in '{file1}': {rows_file1}\")\n",
    "print(f\"Number of rows in '{file2}': {rows_file2}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# File names\n",
    "file1 = \"Final_X_Features.xlsx\"\n",
    "file2 = \"accident_data_2016_2023_subset.xlsx\"\n",
    "\n",
    "# Load the files\n",
    "data1 = pd.read_excel(file1)\n",
    "data2 = pd.read_excel(file2)\n",
    "\n",
    "# Ensure 'Date' columns are in the same format\n",
    "data1[\"Date\"] = pd.to_datetime(data1[\"Date\"]).dt.date\n",
    "data2[\"Date\"] = pd.to_datetime(data2[\"Date\"]).dt.date\n",
    "\n",
    "# Identify dates present in one file but not the other\n",
    "dates_in_file1_not_in_file2 = set(data1[\"Date\"]) - set(data2[\"Date\"])\n",
    "dates_in_file2_not_in_file1 = set(data2[\"Date\"]) - set(data1[\"Date\"])\n",
    "\n",
    "# Display the mismatched dates\n",
    "if dates_in_file1_not_in_file2:\n",
    "    print(\"Dates in Final_X_Features but not in accident_data_2016_2023_subset:\")\n",
    "    print(dates_in_file1_not_in_file2)\n",
    "\n",
    "if dates_in_file2_not_in_file1:\n",
    "    print(\"Dates in accident_data_2016_2023_subset but not in Final_X_Features:\")\n",
    "    print(dates_in_file2_not_in_file1)\n",
    "\n",
    "# Optionally save mismatched dates to files\n",
    "mismatched_dates = {\n",
    "    \"Dates in Final_X_Features but not in accident_data_2016_2023_subset\": list(dates_in_file1_not_in_file2),\n",
    "    \"Dates in accident_data_2016_2023_subset but not in Final_X_Features\": list(dates_in_file2_not_in_file1),\n",
    "}\n",
    "\n",
    "output_file_name = \"mismatched_dates.xlsx\"\n",
    "with pd.ExcelWriter(output_file_name) as writer:\n",
    "    pd.DataFrame({\"Mismatched Dates\": list(dates_in_file1_not_in_file2)}).to_excel(\n",
    "        writer, sheet_name=\"Dates_in_Final_X_Features\", index=False\n",
    "    )\n",
    "    pd.DataFrame({\"Mismatched Dates\": list(dates_in_file2_not_in_file1)}).to_excel(\n",
    "        writer, sheet_name=\"Dates_in_accident_data\", index=False\n",
    "    )\n",
    "\n",
    "print(f\"Mismatched dates have been saved to '{output_file_name}'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the file\n",
    "file_name = \"accident_data_2016_2023_subset.xlsx\"\n",
    "data = pd.read_excel(file_name)\n",
    "\n",
    "# Drop rows where all numeric columns are zero\n",
    "# Only consider numeric columns for the all-zero check\n",
    "numeric_columns = data.select_dtypes(include=['number']).columns\n",
    "cleaned_data = data.loc[~(data[numeric_columns] == 0).all(axis=1)]\n",
    "\n",
    "# Save the cleaned data to a new file\n",
    "output_file_name = \"Final_Y.xlsx\"\n",
    "cleaned_data.to_excel(output_file_name, index=False)\n",
    "\n",
    "print(f\"The cleaned file has been saved as '{output_file_name}'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Load the cleaned file\n",
    "file_name = \"Final_Y.xlsx\"\n",
    "data = pd.read_excel(file_name)\n",
    "\n",
    "# Get the number of rows\n",
    "number_of_rows = len(data)\n",
    "\n",
    "print(f\"The number of rows in '{file_name}' is: {number_of_rows}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Load the file\n",
    "file_name = \"Final_Y.xlsx\"\n",
    "data = pd.read_excel(file_name)\n",
    "\n",
    "# Remove the time part from the Date column\n",
    "data[\"Date\"] = pd.to_datetime(data[\"Date\"]).dt.date\n",
    "\n",
    "# Save the updated data to a temporary file\n",
    "output_file_name = \"Final_Y.xlsx\"\n",
    "data.to_excel(output_file_name, index=False)\n",
    "\n",
    "print(f\"The updated file has been saved as '{output_file_name}'.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# Load the files\n",
    "x_file = \"Final_X_Features.xlsx\"\n",
    "y_file = \"Final_Y.xlsx\"\n",
    "\n",
    "X = pd.read_excel(x_file)\n",
    "Y = pd.read_excel(y_file)\n",
    "\n",
    "# Ensure the rows are aligned by 'Date' if necessary\n",
    "if \"Date\" in X.columns and \"Date\" in Y.columns:\n",
    "    X = X.sort_values(by=\"Date\").reset_index(drop=True)\n",
    "    Y = Y.sort_values(by=\"Date\").reset_index(drop=True)\n",
    "\n",
    "# Shuffle the rows while keeping X and Y aligned\n",
    "X, Y = shuffle(X, Y, random_state=42)\n",
    "\n",
    "# Save the shuffled data back to files for later use\n",
    "X.to_excel(\"Shuffled_X_Features.xlsx\", index=False)\n",
    "Y.to_excel(\"Shuffled_Y.xlsx\", index=False)\n",
    "\n",
    "print(\"Shuffled X and Y have been saved as 'Shuffled_X_Features.xlsx' and 'Shuffled_Y.xlsx'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Building the model)\n",
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor \n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the shuffled datasets\n",
    "X_file = \"Shuffled_X_Features.xlsx\"\n",
    "Y_file = \"Shuffled_Y.xlsx\"\n",
    "\n",
    "X = pd.read_excel(X_file)\n",
    "y = pd.read_excel(Y_file)\n",
    "\n",
    "# Drop the 'Date' column from both X and y, if it exists\n",
    "if \"Date\" in X.columns:\n",
    "    X = X.drop(columns=[\"Date\"])\n",
    "if \"Date\" in y.columns:\n",
    "    y = y.drop(columns=[\"Date\"])\n",
    "\n",
    "# Split the data into training (80%) and testing (20%) sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"Data successfully split into training and testing sets.\")\n",
    "print(f\"Training set size: {X_train.shape[0]} rows\")\n",
    "print(f\"Testing set size: {X_test.shape[0]} rows\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "model = RandomForestRegressor(random_state=42)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print(\"Model training completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)  # Mean Squared Error\n",
    "r2 = r2_score(y_test, y_pred)  # R-squared score\n",
    "\n",
    "# Display the evaluation results\n",
    "print(f\"Model Evaluation:\")\n",
    "print(f\"Mean Squared Error (MSE): {mse}\")\n",
    "print(f\"R-squared (R2 Score): {r2}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Ensure y_test and y_pred are DataFrames for saving\n",
    "y_test_df = pd.DataFrame(y_test).reset_index(drop=True)\n",
    "y_pred_df = pd.DataFrame(y_pred, columns=y_test_df.columns).reset_index(drop=True)\n",
    "\n",
    "# Save to Excel files\n",
    "y_test_file = \"y_test.xlsx\"\n",
    "y_pred_file = \"y_pred.xlsx\"\n",
    "\n",
    "y_test_df.to_excel(y_test_file, index=False)\n",
    "y_pred_df.to_excel(y_pred_file, index=False)\n",
    "\n",
    "print(f\"y_test has been saved to '{y_test_file}'.\")\n",
    "print(f\"y_pred has been saved to '{y_pred_file}'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Create a scatter plot to visualize the predictions vs. actual values\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test, y_pred, alpha=0.6, edgecolor='k')\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], color='red', linestyle='--', linewidth=2)\n",
    "plt.title(\"Predictions vs. Actual Values\", fontsize=14)\n",
    "plt.xlabel(\"Actual Values (y_test)\", fontsize=12)\n",
    "plt.ylabel(\"Predicted Values (y_pred)\", fontsize=12)\n",
    "plt.grid(alpha=0.4)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Ensure y_test and y_pred are flattened and of the same size\n",
    "y_test_flat = np.array(y_test).flatten()  # Convert to a flat array\n",
    "y_pred_flat = np.array(y_pred).flatten()  # Convert to a flat array\n",
    "\n",
    "# Check that sizes match\n",
    "if len(y_test_flat) != len(y_pred_flat):\n",
    "    print(\"Error: y_test and y_pred must have the same size.\")\n",
    "else:\n",
    "    # Create a scatter plot for actual vs predicted values with different colors\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(np.arange(len(y_test_flat)), y_test_flat, color='blue', label='Actual Values', alpha=0.6)\n",
    "    plt.scatter(np.arange(len(y_pred_flat)), y_pred_flat, color='orange', label='Predicted Values', alpha=0.6)\n",
    "    plt.title(\"Actual vs Predicted Values\", fontsize=14)\n",
    "    plt.xlabel(\"Index\", fontsize=12)\n",
    "    plt.ylabel(\"Values\", fontsize=12)\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.grid(alpha=0.4)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Extract feature importance from the trained model\n",
    "feature_importances = model.feature_importances_\n",
    "\n",
    "# Match the importances with feature names\n",
    "feature_names = X_train.columns\n",
    "importance_df = pd.DataFrame({\n",
    "    \"Feature\": feature_names,\n",
    "    \"Importance\": feature_importances\n",
    "}).sort_values(by=\"Importance\", ascending=False)\n",
    "\n",
    "# Display the top features\n",
    "print(\"Top Features by Importance:\")\n",
    "print(importance_df.head(10))  # Show the top 10 features\n",
    "\n",
    "# Plot the feature importance\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.barh(importance_df[\"Feature\"], importance_df[\"Importance\"], color=\"skyblue\")\n",
    "plt.gca().invert_yaxis()  # Invert y-axis to show the most important feature at the top\n",
    "plt.title(\"Feature Importance\", fontsize=14)\n",
    "plt.xlabel(\"Importance\", fontsize=12)\n",
    "plt.ylabel(\"Features\", fontsize=12)\n",
    "plt.grid(alpha=0.4)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Save the trained model to a file with versioning\n",
    "model_file = \"random_forest_model_v1.pkl\"\n",
    "joblib.dump(model, model_file)\n",
    "\n",
    "print(f\"Model has been saved as '{model_file}'.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the hyperparameters grid\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],        # Number of trees in the forest\n",
    "    'max_depth': [None, 10, 20, 30],      # Maximum depth of each tree\n",
    "    'min_samples_split': [2, 5, 10],      # Minimum samples required to split a node\n",
    "    'min_samples_leaf': [1, 2, 4],        # Minimum samples required to be at a leaf node\n",
    "}\n",
    "\n",
    "# Initialize the GridSearchCV\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=RandomForestRegressor(random_state=42),\n",
    "    param_grid=param_grid,\n",
    "    scoring='r2',  # Use R-squared as the evaluation metric\n",
    "    cv=3,          # 3-fold cross-validation\n",
    "    verbose=2,\n",
    "    n_jobs=-1      # Use all available processors\n",
    ")\n",
    "\n",
    "# Perform the grid search on the training data\n",
    "print(\"Starting hyperparameter tuning...\")\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters and best score\n",
    "best_params = grid_search.best_params_\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "print(\"\\nHyperparameter Tuning Completed!\")\n",
    "print(f\"Best Parameters: {best_params}\")\n",
    "print(f\"Best R-squared Score on Training Data: {grid_search.best_score_}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on the test set using the tuned model\n",
    "y_pred_tuned = best_model.predict(X_test)\n",
    "\n",
    "# Evaluate the tuned model\n",
    "mse_tuned = mean_squared_error(y_test, y_pred_tuned)  # Mean Squared Error\n",
    "r2_tuned = r2_score(y_test, y_pred_tuned)  # R-squared Score\n",
    "\n",
    "# Display the evaluation results for the tuned model\n",
    "print(\"Tuned Model Evaluation:\")\n",
    "print(f\"Mean Squared Error (MSE): {mse_tuned}\")\n",
    "print(f\"R-squared (R2 Score): {r2_tuned}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display a comparison of the base and tuned models\n",
    "print(\"\\nComparison of Base Model vs Tuned Model:\")\n",
    "print(f\"Base Model R2 Score: {r2}\")\n",
    "print(f\"Tuned Model R2 Score: {r2_tuned}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the tuned model as version 2\n",
    "tuned_model_file = \"random_forest_model_v2.pkl\"\n",
    "joblib.dump(best_model, tuned_model_file)\n",
    "\n",
    "print(f\"Tuned model has been saved as '{tuned_model_file}'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a threshold for low importance\n",
    "importance_threshold = 0.01  # Features with importance < 0.01 will be removed\n",
    "\n",
    "# Filter features with importance >= threshold\n",
    "important_features = importance_df[importance_df[\"Importance\"] >= importance_threshold][\"Feature\"].tolist()\n",
    "\n",
    "# Create a reduced feature set\n",
    "X_train_reduced = X_train[important_features]\n",
    "X_test_reduced = X_test[important_features]\n",
    "\n",
    "print(f\"Number of important features retained: {len(important_features)}\")\n",
    "print(f\"Number of features removed: {len(X_train.columns) - len(important_features)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a new Random Forest model using reduced features\n",
    "reduced_model = RandomForestRegressor(random_state=42)\n",
    "reduced_model.fit(X_train_reduced, y_train)\n",
    "\n",
    "# Evaluate the reduced model\n",
    "y_pred_reduced = reduced_model.predict(X_test_reduced)\n",
    "\n",
    "mse_reduced = mean_squared_error(y_test, y_pred_reduced)\n",
    "r2_reduced = r2_score(y_test, y_pred_reduced)\n",
    "\n",
    "print(\"\\nReduced Model Evaluation:\")\n",
    "print(f\"Mean Squared Error (MSE): {mse_reduced}\")\n",
    "print(f\"R-squared (R2 Score): {r2_reduced}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nComparison of All Models:\")\n",
    "print(f\"Base Model R2 Score: {r2}\")\n",
    "print(f\"Tuned Model R2 Score: {r2_tuned}\")\n",
    "print(f\"Reduced Model R2 Score: {r2_reduced}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
