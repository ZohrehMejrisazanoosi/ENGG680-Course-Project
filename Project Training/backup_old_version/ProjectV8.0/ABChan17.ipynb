{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and testing split done.\n",
      "Training set: 338 samples\n",
      "Testing set: 146 samples\n",
      "Pipeline initialized.\n",
      "Fitting 5 folds for each of 162 candidates, totalling 810 fits\n",
      "Grid search completed.\n",
      "Best parameters:\n",
      " {'classifier__estimator__class_weight': 'balanced', 'classifier__estimator__max_depth': None, 'classifier__estimator__min_samples_leaf': 5, 'classifier__estimator__min_samples_split': 2, 'classifier__estimator__n_estimators': 500}\n",
      "Best cross-validation train score: 1.00\n",
      "Best cross-validation validation score: 0.64\n",
      "Test-set F1-score: 0.68\n",
      "Classification report on the test set:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.65      0.59        55\n",
      "           1       0.50      0.58      0.54        48\n",
      "           2       0.71      0.80      0.75        75\n",
      "           3       0.61      0.66      0.63        76\n",
      "           4       0.72      0.77      0.74        73\n",
      "           5       0.72      0.73      0.72        93\n",
      "\n",
      "   micro avg       0.65      0.71      0.68       420\n",
      "   macro avg       0.63      0.70      0.66       420\n",
      "weighted avg       0.65      0.71      0.68       420\n",
      " samples avg       0.47      0.56      0.48       420\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('xydata_2021winter.csv')\n",
    "\n",
    "# Define target and features\n",
    "target_columns_clusters = [f\"Cluster{i}\" for i in range(6)]  # Target variables\n",
    "columns_to_drop = target_columns_clusters + [\"Date\", \"Total_Accidents\"]  # Non-predictive columns\n",
    "\n",
    "# Retain only the most relevant features for X\n",
    "selected_features = [\n",
    "     \"Time_Period\", \"Stn Press (kPa)\", \"Dew Point Temp (°C)\", \"Rel Hum (%)\",\n",
    "    \"Visibility (km)\", \"Temp (°C)\", \"Wind Dir (10s deg)\", \"Wind Spd (km/h)\",\n",
    "    \"C0D-1HA\", \"C0D-2HA\", \"C0D-4HA\",  # Cluster 0 historical features\n",
    "    \"C1D-1HA\", \"C1D-2HA\", \"C1D-4HA\",  # Cluster 1 historical features\n",
    "    \"C2D-1HA\", \"C2D-2HA\", \"C2D-3HA\",  # Cluster 2 historical features\n",
    "    \"C3D-1HA\", \"C4D-1HA\", \"C4D-2HA\", \"C5D-1HA\"  # Other cluster features\n",
    "]\n",
    "\n",
    "# Define target and features\n",
    "y = data[target_columns_clusters]  # Binary target\n",
    "X = data[selected_features]  # Selected features\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "print(\"Training and testing split done.\")\n",
    "print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"Testing set: {X_test.shape[0]} samples\")\n",
    "\n",
    "# Step 1: Initialize Pipeline\n",
    "pipe_rf = Pipeline([\n",
    "    ('classifier', MultiOutputClassifier(RandomForestClassifier(random_state=42)))\n",
    "])\n",
    "\n",
    "print(\"Pipeline initialized.\")\n",
    "\n",
    "# {'classifier__estimator__class_weight': 'balanced', 'classifier__estimator__max_depth': None, 'classifier__estimator__min_samples_leaf': 5, 'classifier__estimator__min_samples_split': 2, 'classifier__estimator__n_estimators': 200}\n",
    "\n",
    "# Step 2: Define Extended Parameter Grid\n",
    "param_grid_rf = {\n",
    "    'classifier__estimator__n_estimators': [100, 200, 500],  # Number of trees\n",
    "    'classifier__estimator__max_depth': [None, 10, 20],  # Tree depth\n",
    "    'classifier__estimator__min_samples_split': [2, 5, 10],  # Min samples to split\n",
    "    'classifier__estimator__min_samples_leaf': [1, 2, 5],  # Min samples per leaf\n",
    "    'classifier__estimator__class_weight': [None, 'balanced']  # Handle class imbalance\n",
    "}\n",
    "\n",
    "# Step 3: Perform Grid Search with Cross-Validation\n",
    "grid_rf = GridSearchCV(\n",
    "    pipe_rf, param_grid_rf, cv=5, scoring='f1_weighted', return_train_score=True, verbose=1, n_jobs=-1\n",
    ")\n",
    "grid_rf.fit(X_train, y_train)\n",
    "\n",
    "print(\"Grid search completed.\")\n",
    "\n",
    "# Step 4: Print the Best Parameters\n",
    "print(\"Best parameters:\\n\", grid_rf.best_params_)\n",
    "\n",
    "# Best training and validation scores\n",
    "best_train_score = max(grid_rf.cv_results_['mean_train_score'])\n",
    "print(f\"Best cross-validation train score: {best_train_score:.2f}\")\n",
    "best_val_score = grid_rf.best_score_\n",
    "print(f\"Best cross-validation validation score: {best_val_score:.2f}\")\n",
    "\n",
    "# Step 5: Evaluate on Test Set\n",
    "y_test_pred = grid_rf.best_estimator_.predict(X_test)\n",
    "\n",
    "# Test set F1-score\n",
    "test_f1_score = f1_score(y_test, y_test_pred, average='weighted')\n",
    "print(f\"Test-set F1-score: {test_f1_score:.2f}\")\n",
    "\n",
    "# Step 6: Detailed Evaluation\n",
    "print(\"Classification report on the test set:\\n\")\n",
    "print(classification_report(y_test, y_test_pred, zero_division=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and testing split done.\n",
      "Training set: 338 samples\n",
      "Testing set: 146 samples\n",
      "Pipeline initialized.\n",
      "Fitting 5 folds for each of 81 candidates, totalling 405 fits\n",
      "Grid search completed.\n",
      "Best parameters:\n",
      " {'classifier__estimator__class_weight': 'balanced', 'classifier__estimator__max_depth': 10, 'classifier__estimator__min_samples_leaf': 7, 'classifier__estimator__min_samples_split': 2, 'classifier__estimator__n_estimators': 100}\n",
      "Best cross-validation train score: 0.96\n",
      "Best cross-validation validation score: 0.65\n",
      "Test-set F1-score: 0.68\n",
      "Classification report on the test set:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.71      0.61        55\n",
      "           1       0.45      0.60      0.52        48\n",
      "           2       0.68      0.79      0.73        75\n",
      "           3       0.64      0.64      0.64        76\n",
      "           4       0.74      0.78      0.76        73\n",
      "           5       0.72      0.74      0.73        93\n",
      "\n",
      "   micro avg       0.64      0.72      0.68       420\n",
      "   macro avg       0.63      0.71      0.67       420\n",
      "weighted avg       0.65      0.72      0.68       420\n",
      " samples avg       0.47      0.56      0.48       420\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('xydata_2021winter.csv')\n",
    "\n",
    "# Define target and features\n",
    "target_columns_clusters = [f\"Cluster{i}\" for i in range(6)]  # Target variables\n",
    "columns_to_drop = target_columns_clusters + [\"Date\", \"Total_Accidents\"]  # Non-predictive columns\n",
    "\n",
    "# Retain only the most relevant features for X\n",
    "selected_features = [\n",
    "     \"Time_Period\", \"Stn Press (kPa)\", \"Dew Point Temp (°C)\", \"Rel Hum (%)\",\n",
    "    \"Visibility (km)\", \"Temp (°C)\", \"Wind Dir (10s deg)\", \"Wind Spd (km/h)\",\n",
    "    \"C0D-1HA\", \"C0D-2HA\", \"C0D-4HA\",  # Cluster 0 historical features\n",
    "    \"C1D-1HA\", \"C1D-2HA\", \"C1D-4HA\",  # Cluster 1 historical features\n",
    "    \"C2D-1HA\", \"C2D-2HA\", \"C2D-3HA\",  # Cluster 2 historical features\n",
    "    \"C3D-1HA\", \"C4D-1HA\", \"C4D-2HA\", \"C5D-1HA\"  # Other cluster features\n",
    "]\n",
    "\n",
    "# Define target and features\n",
    "y = data[target_columns_clusters]  # Binary target\n",
    "X = data[selected_features]  # Selected features\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "print(\"Training and testing split done.\")\n",
    "print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"Testing set: {X_test.shape[0]} samples\")\n",
    "\n",
    "# Step 1: Initialize Pipeline\n",
    "pipe_rf = Pipeline([\n",
    "    ('classifier', MultiOutputClassifier(RandomForestClassifier(random_state=42)))\n",
    "])\n",
    "\n",
    "print(\"Pipeline initialized.\")\n",
    "\n",
    "# Step 2: Define Extended Parameter Grid\n",
    "#  {'classifier__estimator__class_weight': 'balanced', 'classifier__estimator__max_depth': None, 'classifier__estimator__min_samples_leaf': 5, 'classifier__estimator__min_samples_split': 2, 'classifier__estimator__n_estimators': 500}\n",
    "\n",
    "param_grid_rf = {\n",
    "    'classifier__estimator__n_estimators': [100, 500, 600],  # Number of trees\n",
    "    'classifier__estimator__max_depth': [10, 12, 15],  # Tree depth\n",
    "    'classifier__estimator__min_samples_split': [2, 3, 5],  # Min samples to split\n",
    "    'classifier__estimator__min_samples_leaf': [3, 5, 7],  # Min samples per leaf\n",
    "    'classifier__estimator__class_weight': ['balanced']  # Handle class imbalance\n",
    "}\n",
    "\n",
    "# Step 3: Perform Grid Search with Cross-Validation\n",
    "grid_rf = GridSearchCV(\n",
    "    pipe_rf, param_grid_rf, cv=5, scoring='f1_weighted', return_train_score=True, verbose=1, n_jobs=-1\n",
    ")\n",
    "grid_rf.fit(X_train, y_train)\n",
    "\n",
    "print(\"Grid search completed.\")\n",
    "\n",
    "# Step 4: Print the Best Parameters\n",
    "print(\"Best parameters:\\n\", grid_rf.best_params_)\n",
    "\n",
    "# Best training and validation scores\n",
    "best_train_score = max(grid_rf.cv_results_['mean_train_score'])\n",
    "print(f\"Best cross-validation train score: {best_train_score:.2f}\")\n",
    "best_val_score = grid_rf.best_score_\n",
    "print(f\"Best cross-validation validation score: {best_val_score:.2f}\")\n",
    "\n",
    "# Step 5: Evaluate on Test Set\n",
    "y_test_pred = grid_rf.best_estimator_.predict(X_test)\n",
    "\n",
    "# Test set F1-score\n",
    "test_f1_score = f1_score(y_test, y_test_pred, average='weighted')\n",
    "print(f\"Test-set F1-score: {test_f1_score:.2f}\")\n",
    "\n",
    "# Step 6: Detailed Evaluation\n",
    "print(\"Classification report on the test set:\\n\")\n",
    "print(classification_report(y_test, y_test_pred, zero_division=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and testing split done.\n",
      "Training set: 338 samples\n",
      "Testing set: 146 samples\n",
      "Pipeline initialized.\n",
      "Fitting 5 folds for each of 81 candidates, totalling 405 fits\n",
      "Grid search completed.\n",
      "Best parameters:\n",
      " {'classifier__estimator__class_weight': 'balanced', 'classifier__estimator__max_depth': 10, 'classifier__estimator__min_samples_leaf': 8, 'classifier__estimator__min_samples_split': 2, 'classifier__estimator__n_estimators': 120}\n",
      "Best cross-validation train score: 0.88\n",
      "Best cross-validation validation score: 0.66\n",
      "Test-set F1-score: 0.68\n",
      "Classification report on the test set:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      0.71      0.61        55\n",
      "           1       0.46      0.60      0.52        48\n",
      "           2       0.69      0.79      0.74        75\n",
      "           3       0.63      0.67      0.65        76\n",
      "           4       0.71      0.75      0.73        73\n",
      "           5       0.72      0.72      0.72        93\n",
      "\n",
      "   micro avg       0.64      0.71      0.67       420\n",
      "   macro avg       0.63      0.71      0.66       420\n",
      "weighted avg       0.64      0.71      0.68       420\n",
      " samples avg       0.45      0.55      0.47       420\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('xydata_2021winter.csv')\n",
    "\n",
    "# Define target and features\n",
    "target_columns_clusters = [f\"Cluster{i}\" for i in range(6)]  # Target variables\n",
    "columns_to_drop = target_columns_clusters + [\"Date\", \"Total_Accidents\"]  # Non-predictive columns\n",
    "\n",
    "# Retain only the most relevant features for X\n",
    "selected_features = [\n",
    "     \"Time_Period\", \"Stn Press (kPa)\", \"Dew Point Temp (°C)\", \"Rel Hum (%)\",\n",
    "    \"Visibility (km)\", \"Temp (°C)\", \"Wind Dir (10s deg)\", \"Wind Spd (km/h)\",\n",
    "    \"C0D-1HA\", \"C0D-2HA\", \"C0D-4HA\",  # Cluster 0 historical features\n",
    "    \"C1D-1HA\", \"C1D-2HA\", \"C1D-4HA\",  # Cluster 1 historical features\n",
    "    \"C2D-1HA\", \"C2D-2HA\", \"C2D-3HA\",  # Cluster 2 historical features\n",
    "    \"C3D-1HA\", \"C4D-1HA\", \"C4D-2HA\", \"C5D-1HA\"  # Other cluster features\n",
    "]\n",
    "\n",
    "# Define target and features\n",
    "y = data[target_columns_clusters]  # Binary target\n",
    "X = data[selected_features]  # Selected features\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "print(\"Training and testing split done.\")\n",
    "print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"Testing set: {X_test.shape[0]} samples\")\n",
    "\n",
    "# Step 1: Initialize Pipeline\n",
    "pipe_rf = Pipeline([\n",
    "    ('classifier', MultiOutputClassifier(RandomForestClassifier(random_state=42)))\n",
    "])\n",
    "\n",
    "print(\"Pipeline initialized.\")\n",
    "\n",
    "# Step 2: Define Extended Parameter Grid\n",
    "# {'classifier__estimator__class_weight': 'balanced', 'classifier__estimator__max_depth': 10, 'classifier__estimator__min_samples_leaf': 7, 'classifier__estimator__min_samples_split': 2, 'classifier__estimator__n_estimators': 100}\n",
    "\n",
    "param_grid_rf = {\n",
    "    'classifier__estimator__n_estimators': [80, 100, 120],\n",
    "    'classifier__estimator__max_depth': [8, 10, 12],\n",
    "    'classifier__estimator__min_samples_split': [2, 4, 6],\n",
    "    'classifier__estimator__min_samples_leaf': [6, 7, 8],\n",
    "    'classifier__estimator__class_weight': ['balanced']\n",
    "}\n",
    "\n",
    "# Step 3: Perform Grid Search with Cross-Validation\n",
    "grid_rf = GridSearchCV(\n",
    "    pipe_rf, param_grid_rf, cv=5, scoring='f1_weighted', return_train_score=True, verbose=1, n_jobs=-1\n",
    ")\n",
    "grid_rf.fit(X_train, y_train)\n",
    "\n",
    "print(\"Grid search completed.\")\n",
    "\n",
    "# Step 4: Print the Best Parameters\n",
    "print(\"Best parameters:\\n\", grid_rf.best_params_)\n",
    "\n",
    "# Best training and validation scores\n",
    "best_train_score = max(grid_rf.cv_results_['mean_train_score'])\n",
    "print(f\"Best cross-validation train score: {best_train_score:.2f}\")\n",
    "best_val_score = grid_rf.best_score_\n",
    "print(f\"Best cross-validation validation score: {best_val_score:.2f}\")\n",
    "\n",
    "# Step 5: Evaluate on Test Set\n",
    "y_test_pred = grid_rf.best_estimator_.predict(X_test)\n",
    "\n",
    "# Test set F1-score\n",
    "test_f1_score = f1_score(y_test, y_test_pred, average='weighted')\n",
    "print(f\"Test-set F1-score: {test_f1_score:.2f}\")\n",
    "\n",
    "# Step 6: Detailed Evaluation\n",
    "print(\"Classification report on the test set:\\n\")\n",
    "print(classification_report(y_test, y_test_pred, zero_division=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and testing split done.\n",
      "Training set: 338 samples\n",
      "Testing set: 146 samples\n",
      "Pipeline initialized.\n",
      "Fitting 5 folds for each of 81 candidates, totalling 405 fits\n",
      "Grid search completed.\n",
      "Best parameters:\n",
      " {'classifier__estimator__class_weight': 'balanced', 'classifier__estimator__max_depth': 9, 'classifier__estimator__min_samples_leaf': 8, 'classifier__estimator__min_samples_split': 2, 'classifier__estimator__n_estimators': 120}\n",
      "Best cross-validation train score: 0.85\n",
      "Best cross-validation validation score: 0.66\n",
      "Test-set F1-score: 0.68\n",
      "Classification report on the test set:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      0.73      0.62        55\n",
      "           1       0.48      0.65      0.55        48\n",
      "           2       0.69      0.79      0.73        75\n",
      "           3       0.64      0.67      0.65        76\n",
      "           4       0.71      0.75      0.73        73\n",
      "           5       0.72      0.72      0.72        93\n",
      "\n",
      "   micro avg       0.64      0.72      0.68       420\n",
      "   macro avg       0.63      0.72      0.67       420\n",
      "weighted avg       0.65      0.72      0.68       420\n",
      " samples avg       0.45      0.56      0.47       420\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('xydata_2021winter.csv')\n",
    "\n",
    "# Define target and features\n",
    "target_columns_clusters = [f\"Cluster{i}\" for i in range(6)]  # Target variables\n",
    "columns_to_drop = target_columns_clusters + [\"Date\", \"Total_Accidents\"]  # Non-predictive columns\n",
    "\n",
    "# Retain only the most relevant features for X\n",
    "selected_features = [\n",
    "     \"Time_Period\", \"Stn Press (kPa)\", \"Dew Point Temp (°C)\", \"Rel Hum (%)\",\n",
    "    \"Visibility (km)\", \"Temp (°C)\", \"Wind Dir (10s deg)\", \"Wind Spd (km/h)\",\n",
    "    \"C0D-1HA\", \"C0D-2HA\", \"C0D-4HA\",  # Cluster 0 historical features\n",
    "    \"C1D-1HA\", \"C1D-2HA\", \"C1D-4HA\",  # Cluster 1 historical features\n",
    "    \"C2D-1HA\", \"C2D-2HA\", \"C2D-3HA\",  # Cluster 2 historical features\n",
    "    \"C3D-1HA\", \"C4D-1HA\", \"C4D-2HA\", \"C5D-1HA\"  # Other cluster features\n",
    "]\n",
    "\n",
    "# Define target and features\n",
    "y = data[target_columns_clusters]  # Binary target\n",
    "X = data[selected_features]  # Selected features\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "print(\"Training and testing split done.\")\n",
    "print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"Testing set: {X_test.shape[0]} samples\")\n",
    "\n",
    "# Step 1: Initialize Pipeline\n",
    "pipe_rf = Pipeline([\n",
    "    ('classifier', MultiOutputClassifier(RandomForestClassifier(random_state=42)))\n",
    "])\n",
    "\n",
    "print(\"Pipeline initialized.\")\n",
    "\n",
    "# Step 2: Define Extended Parameter Grid\n",
    "# {'classifier__estimator__class_weight': 'balanced', 'classifier__estimator__max_depth': 10, 'classifier__estimator__min_samples_leaf': 7, 'classifier__estimator__min_samples_split': 2, 'classifier__estimator__n_estimators': 100}\n",
    "\n",
    "param_grid_rf = {\n",
    "    'classifier__estimator__n_estimators': [110, 120, 130],\n",
    "    'classifier__estimator__max_depth': [9, 10, 11],\n",
    "    'classifier__estimator__min_samples_split': [2, 3, 4],\n",
    "    'classifier__estimator__min_samples_leaf': [7, 8, 9],\n",
    "    'classifier__estimator__class_weight': ['balanced']\n",
    "}\n",
    "\n",
    "# Step 3: Perform Grid Search with Cross-Validation\n",
    "grid_rf = GridSearchCV(\n",
    "    pipe_rf, param_grid_rf, cv=5, scoring='f1_weighted', return_train_score=True, verbose=1, n_jobs=-1\n",
    ")\n",
    "grid_rf.fit(X_train, y_train)\n",
    "\n",
    "print(\"Grid search completed.\")\n",
    "\n",
    "# Step 4: Print the Best Parameters\n",
    "print(\"Best parameters:\\n\", grid_rf.best_params_)\n",
    "\n",
    "# Best training and validation scores\n",
    "best_train_score = max(grid_rf.cv_results_['mean_train_score'])\n",
    "print(f\"Best cross-validation train score: {best_train_score:.2f}\")\n",
    "best_val_score = grid_rf.best_score_\n",
    "print(f\"Best cross-validation validation score: {best_val_score:.2f}\")\n",
    "\n",
    "# Step 5: Evaluate on Test Set\n",
    "y_test_pred = grid_rf.best_estimator_.predict(X_test)\n",
    "\n",
    "# Test set F1-score\n",
    "test_f1_score = f1_score(y_test, y_test_pred, average='weighted')\n",
    "print(f\"Test-set F1-score: {test_f1_score:.2f}\")\n",
    "\n",
    "# Step 6: Detailed Evaluation\n",
    "print(\"Classification report on the test set:\\n\")\n",
    "print(classification_report(y_test, y_test_pred, zero_division=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and testing split done.\n",
      "Training set: 338 samples\n",
      "Testing set: 146 samples\n",
      "Pipeline initialized.\n",
      "Fitting 5 folds for each of 54 candidates, totalling 270 fits\n",
      "Grid search completed.\n",
      "Best parameters:\n",
      " {'classifier__estimator__class_weight': 'balanced', 'classifier__estimator__max_depth': 9, 'classifier__estimator__min_samples_leaf': 8, 'classifier__estimator__min_samples_split': 2, 'classifier__estimator__n_estimators': 135}\n",
      "Best cross-validation train score: 0.85\n",
      "Best cross-validation validation score: 0.66\n",
      "Test-set F1-score: 0.69\n",
      "Classification report on the test set:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      0.71      0.61        55\n",
      "           1       0.48      0.67      0.56        48\n",
      "           2       0.69      0.81      0.75        75\n",
      "           3       0.64      0.68      0.66        76\n",
      "           4       0.72      0.77      0.74        73\n",
      "           5       0.72      0.72      0.72        93\n",
      "\n",
      "   micro avg       0.64      0.73      0.68       420\n",
      "   macro avg       0.63      0.73      0.67       420\n",
      "weighted avg       0.65      0.73      0.69       420\n",
      " samples avg       0.46      0.56      0.48       420\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('xydata_2021winter.csv')\n",
    "\n",
    "# Define target and features\n",
    "target_columns_clusters = [f\"Cluster{i}\" for i in range(6)]  # Target variables\n",
    "columns_to_drop = target_columns_clusters + [\"Date\", \"Total_Accidents\"]  # Non-predictive columns\n",
    "\n",
    "# Retain only the most relevant features for X\n",
    "selected_features = [\n",
    "     \"Time_Period\", \"Stn Press (kPa)\", \"Dew Point Temp (°C)\", \"Rel Hum (%)\",\n",
    "    \"Visibility (km)\", \"Temp (°C)\", \"Wind Dir (10s deg)\", \"Wind Spd (km/h)\",\n",
    "    \"C0D-1HA\", \"C0D-2HA\", \"C0D-4HA\",  # Cluster 0 historical features\n",
    "    \"C1D-1HA\", \"C1D-2HA\", \"C1D-4HA\",  # Cluster 1 historical features\n",
    "    \"C2D-1HA\", \"C2D-2HA\", \"C2D-3HA\",  # Cluster 2 historical features\n",
    "    \"C3D-1HA\", \"C4D-1HA\", \"C4D-2HA\", \"C5D-1HA\"  # Other cluster features\n",
    "]\n",
    "\n",
    "# Define target and features\n",
    "y = data[target_columns_clusters]  # Binary target\n",
    "X = data[selected_features]  # Selected features\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "print(\"Training and testing split done.\")\n",
    "print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"Testing set: {X_test.shape[0]} samples\")\n",
    "\n",
    "# Step 1: Initialize Pipeline\n",
    "pipe_rf = Pipeline([\n",
    "    ('classifier', MultiOutputClassifier(RandomForestClassifier(random_state=42)))\n",
    "])\n",
    "\n",
    "print(\"Pipeline initialized.\")\n",
    "\n",
    "# Step 2: Define Extended Parameter Grid\n",
    "# {'classifier__estimator__class_weight': 'balanced', 'classifier__estimator__max_depth': 10, 'classifier__estimator__min_samples_leaf': 7, 'classifier__estimator__min_samples_split': 2, 'classifier__estimator__n_estimators': 100}\n",
    "\n",
    "param_grid_rf = {\n",
    "'classifier__estimator__n_estimators': [115, 120, 135],\n",
    "    'classifier__estimator__max_depth': [8, 9, 10],\n",
    "    'classifier__estimator__min_samples_split': [2, 3],\n",
    "    'classifier__estimator__min_samples_leaf': [7, 8, 9],\n",
    "    'classifier__estimator__class_weight': ['balanced']\n",
    "}\n",
    "\n",
    "# Step 3: Perform Grid Search with Cross-Validation\n",
    "grid_rf = GridSearchCV(\n",
    "    pipe_rf, param_grid_rf, cv=5, scoring='f1_weighted', return_train_score=True, verbose=1, n_jobs=-1\n",
    ")\n",
    "grid_rf.fit(X_train, y_train)\n",
    "\n",
    "print(\"Grid search completed.\")\n",
    "\n",
    "# Step 4: Print the Best Parameters\n",
    "print(\"Best parameters:\\n\", grid_rf.best_params_)\n",
    "\n",
    "# Best training and validation scores\n",
    "best_train_score = max(grid_rf.cv_results_['mean_train_score'])\n",
    "print(f\"Best cross-validation train score: {best_train_score:.2f}\")\n",
    "best_val_score = grid_rf.best_score_\n",
    "print(f\"Best cross-validation validation score: {best_val_score:.2f}\")\n",
    "\n",
    "# Step 5: Evaluate on Test Set\n",
    "y_test_pred = grid_rf.best_estimator_.predict(X_test)\n",
    "\n",
    "# Test set F1-score\n",
    "test_f1_score = f1_score(y_test, y_test_pred, average='weighted')\n",
    "print(f\"Test-set F1-score: {test_f1_score:.2f}\")\n",
    "\n",
    "# Step 6: Detailed Evaluation\n",
    "print(\"Classification report on the test set:\\n\")\n",
    "print(classification_report(y_test, y_test_pred, zero_division=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and testing split done.\n",
      "Training set: 338 samples\n",
      "Testing set: 146 samples\n",
      "Pipeline initialized.\n",
      "Fitting 5 folds for each of 54 candidates, totalling 270 fits\n",
      "Grid search completed.\n",
      "Best parameters:\n",
      " {'classifier__estimator__class_weight': 'balanced', 'classifier__estimator__max_depth': 10, 'classifier__estimator__min_samples_leaf': 7, 'classifier__estimator__min_samples_split': 2, 'classifier__estimator__n_estimators': 135}\n",
      "Best cross-validation train score: 0.84\n",
      "Best cross-validation validation score: 0.67\n",
      "Test-set F1-score: 0.69\n",
      "Classification report on the test set:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.61      0.60        56\n",
      "           1       0.44      0.61      0.51        41\n",
      "           2       0.61      0.67      0.64        72\n",
      "           3       0.68      0.89      0.77        75\n",
      "           4       0.74      0.72      0.73        76\n",
      "           5       0.70      0.85      0.77        81\n",
      "\n",
      "   micro avg       0.64      0.74      0.69       401\n",
      "   macro avg       0.63      0.73      0.67       401\n",
      "weighted avg       0.65      0.74      0.69       401\n",
      " samples avg       0.44      0.52      0.46       401\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('xydata_2022winter.csv')\n",
    "\n",
    "# Define target and features\n",
    "target_columns_clusters = [f\"Cluster{i}\" for i in range(6)]  # Target variables\n",
    "columns_to_drop = target_columns_clusters + [\"Date\", \"Total_Accidents\"]  # Non-predictive columns\n",
    "\n",
    "# Retain only the most relevant features for X\n",
    "selected_features = [\n",
    "     \"Time_Period\", \"Stn Press (kPa)\", \"Dew Point Temp (°C)\", \"Rel Hum (%)\",\n",
    "    \"Visibility (km)\", \"Temp (°C)\", \"Wind Dir (10s deg)\", \"Wind Spd (km/h)\",\n",
    "    \"C0D-1HA\", \"C0D-2HA\", \"C0D-4HA\",  # Cluster 0 historical features\n",
    "    \"C1D-1HA\", \"C1D-2HA\", \"C1D-4HA\",  # Cluster 1 historical features\n",
    "    \"C2D-1HA\", \"C2D-2HA\", \"C2D-3HA\",  # Cluster 2 historical features\n",
    "    \"C3D-1HA\", \"C4D-1HA\", \"C4D-2HA\", \"C5D-1HA\"  # Other cluster features\n",
    "]\n",
    "\n",
    "# Define target and features\n",
    "y = data[target_columns_clusters]  # Binary target\n",
    "X = data[selected_features]  # Selected features\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "print(\"Training and testing split done.\")\n",
    "print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"Testing set: {X_test.shape[0]} samples\")\n",
    "\n",
    "# Step 1: Initialize Pipeline\n",
    "pipe_rf = Pipeline([\n",
    "    ('classifier', MultiOutputClassifier(RandomForestClassifier(random_state=42)))\n",
    "])\n",
    "\n",
    "print(\"Pipeline initialized.\")\n",
    "\n",
    "# Step 2: Define Extended Parameter Grid\n",
    "# {'classifier__estimator__class_weight': 'balanced', 'classifier__estimator__max_depth': 10, 'classifier__estimator__min_samples_leaf': 7, 'classifier__estimator__min_samples_split': 2, 'classifier__estimator__n_estimators': 100}\n",
    "\n",
    "param_grid_rf = {\n",
    "'classifier__estimator__n_estimators': [115, 120, 135],\n",
    "    'classifier__estimator__max_depth': [8, 9, 10],\n",
    "    'classifier__estimator__min_samples_split': [2, 3],\n",
    "    'classifier__estimator__min_samples_leaf': [7, 8, 9],\n",
    "    'classifier__estimator__class_weight': ['balanced']\n",
    "}\n",
    "\n",
    "# Step 3: Perform Grid Search with Cross-Validation\n",
    "grid_rf = GridSearchCV(\n",
    "    pipe_rf, param_grid_rf, cv=5, scoring='f1_weighted', return_train_score=True, verbose=1, n_jobs=-1\n",
    ")\n",
    "grid_rf.fit(X_train, y_train)\n",
    "\n",
    "print(\"Grid search completed.\")\n",
    "\n",
    "# Step 4: Print the Best Parameters\n",
    "print(\"Best parameters:\\n\", grid_rf.best_params_)\n",
    "\n",
    "# Best training and validation scores\n",
    "best_train_score = max(grid_rf.cv_results_['mean_train_score'])\n",
    "print(f\"Best cross-validation train score: {best_train_score:.2f}\")\n",
    "best_val_score = grid_rf.best_score_\n",
    "print(f\"Best cross-validation validation score: {best_val_score:.2f}\")\n",
    "\n",
    "# Step 5: Evaluate on Test Set\n",
    "y_test_pred = grid_rf.best_estimator_.predict(X_test)\n",
    "\n",
    "# Test set F1-score\n",
    "test_f1_score = f1_score(y_test, y_test_pred, average='weighted')\n",
    "print(f\"Test-set F1-score: {test_f1_score:.2f}\")\n",
    "\n",
    "# Step 6: Detailed Evaluation\n",
    "print(\"Classification report on the test set:\\n\")\n",
    "print(classification_report(y_test, y_test_pred, zero_division=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best cross-validation train score: 0.69\n",
      "Test-set F1-score: 0.72\n",
      "Classification report on the test set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.78      0.64        55\n",
      "           1       0.44      0.67      0.53        48\n",
      "           2       0.70      0.91      0.79        75\n",
      "           3       0.62      0.79      0.69        76\n",
      "           4       0.69      0.86      0.77        73\n",
      "           5       0.72      0.85      0.78        93\n",
      "\n",
      "   micro avg       0.63      0.82      0.71       420\n",
      "   macro avg       0.62      0.81      0.70       420\n",
      "weighted avg       0.64      0.82      0.72       420\n",
      " samples avg       0.70      0.77      0.63       420\n",
      "\n",
      "Model saved successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import joblib\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('xydata_2021winter.csv')\n",
    "\n",
    "# Define target and features\n",
    "target_columns_clusters = [f\"Cluster{i}\" for i in range(6)]  # Target variables\n",
    "columns_to_drop = target_columns_clusters + [\"Date\", \"Total_Accidents\"]  # Non-predictive columns\n",
    "\n",
    "# Retain only the most relevant features for X\n",
    "selected_features = [\n",
    "     \"Time_Period\", \"Stn Press (kPa)\", \"Dew Point Temp (°C)\", \"Rel Hum (%)\",\n",
    "    \"Visibility (km)\", \"Temp (°C)\", \"Wind Dir (10s deg)\", \"Wind Spd (km/h)\",\n",
    "    \"C0D-1HA\", \"C0D-2HA\", \"C0D-4HA\",  # Cluster 0 historical features\n",
    "    \"C1D-1HA\", \"C1D-2HA\", \"C1D-4HA\",  # Cluster 1 historical features\n",
    "    \"C2D-1HA\", \"C2D-2HA\", \"C2D-3HA\",  # Cluster 2 historical features\n",
    "    \"C3D-1HA\", \"C4D-1HA\", \"C4D-2HA\", \"C5D-1HA\"  # Other cluster features\n",
    "]\n",
    "\n",
    "# Define target and features\n",
    "y = data[target_columns_clusters]  # Binary targets\n",
    "X = data[selected_features]  # Selected features\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# Setup the RandomForestClassifier with specific parameters\n",
    "rf_classifier = RandomForestClassifier(\n",
    "    n_estimators=135,\n",
    "    max_depth=9,\n",
    "    min_samples_split=2,\n",
    "    min_samples_leaf=8,\n",
    "    class_weight='balanced',\n",
    "    random_state=42  # for reproducibility\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model using cross-validation on the training set\n",
    "cv_scores = cross_val_score(rf_classifier, X_train, y_train, cv=5, scoring='f1_weighted')\n",
    "print(f\"Best cross-validation train score: {cv_scores.mean():.2f}\")\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = rf_classifier.predict(X_test)\n",
    "\n",
    "# Calculate F1-score on the test set, handling division by zero explicitly\n",
    "test_f1 = f1_score(y_test, y_pred, average='weighted', zero_division=1)\n",
    "print(f\"Test-set F1-score: {test_f1:.2f}\")\n",
    "\n",
    "# Generate a classification report, handling division by zero explicitly\n",
    "print(\"Classification report on the test set:\")\n",
    "print(classification_report(y_test, y_pred, zero_division=1))\n",
    "\n",
    "# Save the model to disk\n",
    "joblib.dump(rf_classifier, 'final_random_forest_model.pkl')\n",
    "print(\"Model saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the trained model...\n",
      "Loading the dataset...\n",
      "Making predictions...\n",
      "Test set accuracy score: 0.19\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Cluster0       0.41      0.69      0.52       144\n",
      "    Cluster1       0.38      0.59      0.46       134\n",
      "    Cluster2       0.55      0.83      0.66       206\n",
      "    Cluster3       0.63      0.80      0.70       246\n",
      "    Cluster4       0.63      0.86      0.73       232\n",
      "    Cluster5       0.66      0.91      0.76       258\n",
      "\n",
      "   micro avg       0.56      0.80      0.66      1220\n",
      "   macro avg       0.54      0.78      0.64      1220\n",
      "weighted avg       0.57      0.80      0.66      1220\n",
      " samples avg       0.42      0.60      0.46      1220\n",
      "\n",
      "\n",
      "First 5 Predictions vs. True Labels:\n",
      "  Predicted                                                  True           \\\n",
      "   Cluster0 Cluster1 Cluster2 Cluster3 Cluster4 Cluster5 Cluster0 Cluster1   \n",
      "0         0        0        1        0        0        1        0        0   \n",
      "1         1        1        1        1        1        1        1        0   \n",
      "2         1        1        1        1        1        1        0        1   \n",
      "3         0        0        0        0        0        0        0        0   \n",
      "4         1        1        1        0        1        1        1        1   \n",
      "\n",
      "                                       \n",
      "  Cluster2 Cluster3 Cluster4 Cluster5  \n",
      "0        0        0        0        1  \n",
      "1        1        0        0        0  \n",
      "2        1        1        1        1  \n",
      "3        0        1        0        1  \n",
      "4        1        0        1        0  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import joblib\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Load the saved model\n",
    "print(\"Loading the trained model...\")\n",
    "model = joblib.load('final_random_forest_model.pkl')\n",
    "\n",
    "# Load the new dataset formatted like the training data\n",
    "print(\"Loading the dataset...\")\n",
    "data = pd.read_csv('xydata_2018winter.csv')\n",
    "\n",
    "# Define the feature columns (ensure they match those used during training)\n",
    "features = [\n",
    "    \"Time_Period\", \"Stn Press (kPa)\", \"Dew Point Temp (°C)\", \"Rel Hum (%)\",\n",
    "    \"Visibility (km)\", \"Temp (°C)\", \"Wind Dir (10s deg)\", \"Wind Spd (km/h)\",\n",
    "    \"C0D-1HA\", \"C0D-2HA\", \"C0D-4HA\",  # Cluster 0 historical features\n",
    "    \"C1D-1HA\", \"C1D-2HA\", \"C1D-4HA\",  # Cluster 1 historical features\n",
    "    \"C2D-1HA\", \"C2D-2HA\", \"C2D-3HA\",  # Cluster 2 historical features\n",
    "    \"C3D-1HA\", \"C4D-1HA\", \"C4D-2HA\", \"C5D-1HA\"  # Other cluster features\n",
    "]\n",
    "\n",
    "# Define the target cluster columns\n",
    "target_columns = [f\"Cluster{i}\" for i in range(6)]\n",
    "\n",
    "# Ensure all features used during training are present in the dataset\n",
    "for feature in features:\n",
    "    if feature not in data.columns:\n",
    "        data[feature] = 0  # Add missing features with a default value of 0\n",
    "\n",
    "# Extract the features (X) and target (y) from the dataset\n",
    "X = data[features]\n",
    "y_true = data[target_columns]\n",
    "\n",
    "# Generate predictions using the loaded model\n",
    "print(\"Making predictions...\")\n",
    "y_pred = model.predict(X)\n",
    "\n",
    "# Convert predictions to a DataFrame for better readability\n",
    "y_pred_df = pd.DataFrame(y_pred, columns=target_columns)\n",
    "\n",
    "# Calculate the accuracy score\n",
    "accuracy = accuracy_score(y_true, y_pred_df)\n",
    "print(f\"Test set accuracy score: {accuracy:.2f}\")\n",
    "\n",
    "# Display a detailed classification report with zero_division=0\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_true, y_pred_df, target_names=target_columns, zero_division=0))\n",
    "\n",
    "# Display the first 5 rows of predictions alongside true labels for comparison\n",
    "print(\"\\nFirst 5 Predictions vs. True Labels:\")\n",
    "comparison = pd.concat([y_pred_df.head(5), y_true.head(5)], axis=1, keys=[\"Predicted\", \"True\"])\n",
    "print(comparison)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
