{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import folium\n",
    "from sklearn.cluster import KMeans\n",
    "from shapely.geometry import Point\n",
    "from scipy.spatial import ConvexHull\n",
    "from itertools import product\n",
    "from datetime import timedelta\n",
    "\n",
    "# Load the dataset\n",
    "traffic_df = pd.read_csv('Traffic_Incidents_20241114.csv')  # Adjust this path if necessary\n",
    "# Use KMeans clustering to \n",
    "n_clusters = 6\n",
    "# set these according to your actual data\n",
    "n_history_days = 4\n",
    "\n",
    "# Options: 'manual' for predefined mapping or 'unsupervised' for clustering\n",
    "cleaning_method = 'unsupervised'  # Change to 'unsupervised' to use clustering-based cleaning\n",
    "\n",
    "# Enable or disable debug mode\n",
    "debug_mode = False  # Set to False to disable debugging outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\Traffic Incidents Data Covers from 2016-12-06 to 2024-11-14\n",
      "Missing Dates or no accidence date List (first 10):\n",
      " [datetime.date(2016, 12, 21), datetime.date(2016, 12, 22), datetime.date(2016, 12, 23), datetime.date(2016, 12, 24), datetime.date(2016, 12, 25), datetime.date(2016, 12, 26), datetime.date(2016, 12, 27), datetime.date(2016, 12, 28), datetime.date(2016, 12, 29), datetime.date(2016, 12, 30)]\n",
      "Recorded Days: 2744\n",
      "All Days: 2901\n",
      "Missing Days: 157\n",
      "\n",
      "Confirmed: No rows exist for the identified missing dates.\n",
      "\n",
      "Missing Values Summary:\n",
      "                      Column  Missing Values  Percentage Missing\n",
      "QUADRANT            QUADRANT           14059           27.693187\n",
      "MODIFIED_DT      MODIFIED_DT           14057           27.689247\n",
      "DESCRIPTION      DESCRIPTION               2            0.003940\n",
      "Date                    Date               0            0.000000\n",
      "Time                    Time               0            0.000000\n",
      "INCIDENT INFO  INCIDENT INFO               0            0.000000\n",
      "START_DT            START_DT               0            0.000000\n",
      "Longitude          Longitude               0            0.000000\n",
      "Latitude            Latitude               0            0.000000\n",
      "Count                  Count               0            0.000000\n",
      "id                        id               0            0.000000\n",
      "Point                  Point               0            0.000000\n"
     ]
    }
   ],
   "source": [
    "# Ensure START_DT column is in datetime format\n",
    "traffic_df['START_DT'] = pd.to_datetime(traffic_df['START_DT'], errors='coerce')  # Handle potential parsing errors\n",
    "\n",
    "# Create separate Date and Time columns\n",
    "traffic_df['Date'] = traffic_df['START_DT'].dt.date  # Extract the date\n",
    "traffic_df['Time'] = traffic_df['START_DT'].dt.strftime('%H:%M:%S')  # Extract the time in 24-hour format as string\n",
    "traffic_df['Date'] = pd.to_datetime(traffic_df['Date'])\n",
    "# Sort by START_DT to ensure both date and time are in order\n",
    "traffic_df = traffic_df.sort_values(by='START_DT', ascending=True)\n",
    "\n",
    "# Rearrange columns to place Date and Time after START_DT (optional)\n",
    "columns_order = ['Date', 'Time'] + [col for col in traffic_df.columns if col not in ['Date', 'Time']]\n",
    "traffic_df = traffic_df[columns_order]\n",
    "\n",
    "# Determine the start and end START_DT of the filtered data\n",
    "start_date = traffic_df['Date'].min().date()\n",
    "end_date = traffic_df['Date'].max().date()\n",
    "print(f\"\\Traffic Incidents Data Covers from {start_date} to {end_date}\")\n",
    "\n",
    "# Generate a complete date range\n",
    "all_dates = pd.date_range(start=start_date, end=end_date).date\n",
    "# Identify recorded dates in the dataset\n",
    "recorded_dates = set(traffic_df['Date'].dt.date)\n",
    "# Identify missing dates\n",
    "missing_dates = sorted(set(all_dates) - recorded_dates)\n",
    "\n",
    "if len(missing_dates) > 0:\n",
    "    print(f\"Missing Dates or no accidence date List (first 10):\\n {missing_dates[:10]}\")\n",
    "\n",
    "# Count missing and recorded dates\n",
    "print(f\"Recorded Days: {len(recorded_dates)}\")\n",
    "print(f\"All Days: {len(all_dates)}\")\n",
    "print(f\"Missing Days: {len(missing_dates)}\")\n",
    "\n",
    "# Verify that no rows exist for the identified missing dates\n",
    "missing_rows = traffic_df[traffic_df['Date'].dt.date.isin(missing_dates)]\n",
    "\n",
    "if not missing_rows.empty:\n",
    "    print(\"\\nUnexpectedly Found Rows for Missing Dates:\")\n",
    "    print(missing_rows)\n",
    "else:\n",
    "    print(\"\\nConfirmed: No rows exist for the identified missing dates.\")\n",
    "\n",
    "# Check for missing values in each column  \n",
    "missing_values = traffic_df.isnull().sum()\n",
    "total_rows = len(traffic_df)\n",
    "\n",
    "# Create a DataFrame for a clear overview\n",
    "missing_summary = pd.DataFrame({\n",
    "    'Column': traffic_df.columns,\n",
    "    'Missing Values': missing_values,\n",
    "    'Percentage Missing': (missing_values / total_rows) * 100\n",
    "})\n",
    "\n",
    "# Display missing summary\n",
    "missing_summary.sort_values(by='Percentage Missing', ascending=False, inplace=True)\n",
    "print(\"\\nMissing Values Summary:\")\n",
    "print(missing_summary)\n",
    "    \n",
    "# Save the modified DataFrame to a CSV file\n",
    "if debug_mode:\n",
    "    traffic_df.to_csv(\"justToCheck1.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data cleaning logic based on the selected method\n",
    "if cleaning_method == 'manual':\n",
    "    print(\"Running manual data cleaning...\")\n",
    "\n",
    "    # Define the center point (longitude, latitude) for Calgary (example coordinates)\n",
    "    CENTER_LONGITUDE = -114.0719\n",
    "    CENTER_LATITUDE = 51.0447\n",
    "\n",
    "    # Function to determine the quadrant based on longitude and latitude\n",
    "    def determine_quadrant(longitude, latitude):\n",
    "        if pd.isnull(longitude) or pd.isnull(latitude):\n",
    "            return np.nan  # Cannot determine if longitude/latitude is missing\n",
    "        if latitude >= CENTER_LATITUDE and longitude <= CENTER_LONGITUDE:\n",
    "            return \"NW\"\n",
    "        elif latitude >= CENTER_LATITUDE and longitude > CENTER_LONGITUDE:\n",
    "            return \"NE\"\n",
    "        elif latitude < CENTER_LATITUDE and longitude <= CENTER_LONGITUDE:\n",
    "            return \"SW\"\n",
    "        elif latitude < CENTER_LATITUDE and longitude > CENTER_LONGITUDE:\n",
    "            return \"SE\"\n",
    "\n",
    "    # Fill in missing QUADRANT values\n",
    "    traffic_df.loc[:, 'QUADRANT'] = traffic_df.apply(\n",
    "        lambda row: determine_quadrant(row['Longitude'], row['Latitude'])\n",
    "        if pd.isnull(row['QUADRANT']) else row['QUADRANT'],\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    # Verify if all missing values in QUADRANT have been handled\n",
    "    missing_quadrants_after = traffic_df['QUADRANT'].isnull().sum()\n",
    "    print(f\"Remaining missing QUADRANT values: {missing_quadrants_after}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running unsupervised clustering for data cleaning...\n",
      "Map saved as 'Traffic_Map_Detailed.html'\n"
     ]
    }
   ],
   "source": [
    "# Data cleaning logic based on the selected method\n",
    "if cleaning_method == 'unsupervised':\n",
    "    print(\"Running unsupervised clustering for data cleaning...\")\n",
    "\n",
    "    kmeans = KMeans(n_clusters, random_state=0)\n",
    "    traffic_df['Cluster_KMeans'] = kmeans.fit_predict(traffic_df[['Latitude', 'Longitude']])\n",
    "\n",
    "    # Create a map to visualize clusters\n",
    "    map_houston = folium.Map(location=[traffic_df['Latitude'].mean(), traffic_df['Longitude'].mean()], zoom_start=11)\n",
    "    colors = ['#3186cc', '#FF5733', '#33FF57', '#f1c40f', '#8e44ad', '#3498db', '#e74c3c']\n",
    "    color_dict = {k: colors[k % len(colors)] for k in range(n_clusters)}\n",
    "\n",
    "    for cluster_id in range(n_clusters):\n",
    "        cluster_points = traffic_df[traffic_df['Cluster_KMeans'] == cluster_id]\n",
    "        points = cluster_points[['Longitude', 'Latitude']].values\n",
    "\n",
    "        if len(points) > 2:\n",
    "            hull = ConvexHull(points)\n",
    "            hull_points = [(points[vertex][1], points[vertex][0]) for vertex in hull.vertices]\n",
    "            folium.Polygon(locations=hull_points,\n",
    "                        color=color_dict[cluster_id],\n",
    "                        fill=True,\n",
    "                        fill_color=color_dict[cluster_id],\n",
    "                        fill_opacity=0.5,\n",
    "                        tooltip=f'Cluster {cluster_id}, Points: {len(points)}').add_to(map_houston)\n",
    "\n",
    "        for _, row in cluster_points.iterrows():\n",
    "            folium.CircleMarker(location=(row['Latitude'], row['Longitude']),\n",
    "                                radius=2,\n",
    "                                color=color_dict[cluster_id],\n",
    "                                fill=True,\n",
    "                                fill_opacity=0.7).add_to(map_houston)\n",
    "\n",
    "    map_houston.save(\"Traffic_Map_Detailed.html\")\n",
    "    print(\"Map saved as 'Traffic_Map_Detailed.html'\")\n",
    "\n",
    "    # Save the modified DataFrame to a CSV file\n",
    "    if debug_mode:\n",
    "        traffic_df.to_csv(\"justToCheck2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classified Incident Types and Their Counts:\n",
      "Incident_Type\n",
      "3    15645\n",
      "1    15096\n",
      "6     9328\n",
      "4     4915\n",
      "2     3917\n",
      "5     1866\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Function to classify descriptions into simplified categories\n",
    "def classify_description(description):\n",
    "    if isinstance(description, str):  # Check if the value is a string\n",
    "        description = description.lower()  # Convert to lowercase for consistency\n",
    "        if \"single vehicle\" in description:\n",
    "            return 2  # Single vehicle incident\n",
    "        elif \"two vehicle\" in description or \"2 vehicle\" in description:\n",
    "            return 3  # Two vehicle incident\n",
    "        elif \"multi-vehicle\" in description or \"multi vehicle\" in description:\n",
    "            return 4  # Multi-vehicle incident\n",
    "        elif \"pedestrian\" in description:\n",
    "            return 5  # Pedestrian involved\n",
    "        elif \"stalled\" in description or \"blocking\" in description:\n",
    "            return 6  # Stalled or obstructive\n",
    "        else:\n",
    "            return 1  # Other/Unknown\n",
    "    else:\n",
    "        return 1  # Default for non-string values\n",
    "\n",
    "# Apply classification function to the DESCRIPTION column\n",
    "traffic_df['Incident_Type'] = traffic_df['DESCRIPTION'].apply(classify_description)\n",
    "\n",
    "# Verify the classification\n",
    "incident_type_counts = traffic_df['Incident_Type'].value_counts()\n",
    "\n",
    "# Display the counts of each classified type\n",
    "print(\"\\nClassified Incident Types and Their Counts:\")\n",
    "print(incident_type_counts)\n",
    "\n",
    "# Save the modified DataFrame to a CSV file\n",
    "if debug_mode:\n",
    "    traffic_df.to_csv(\"justToCheck3.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Modified DataFrame with 'Time_Period':\n",
      "                 START_DT  Time_Period\n",
      "6413  2016-12-06 10:00:00            0\n",
      "17172 2016-12-06 14:36:00            1\n",
      "8955  2016-12-06 16:25:00            1\n",
      "32623 2016-12-06 16:26:00            1\n",
      "32280 2016-12-06 17:05:00            1\n"
     ]
    }
   ],
   "source": [
    "# Define time period mapping function with numeric values\n",
    "def assign_time_period(hour):\n",
    "    if 6 <= hour < 12:\n",
    "        return 0  # Morning\n",
    "    elif 12 <= hour < 18:\n",
    "        return 1  # Lunch\n",
    "    elif 18 <= hour < 24:\n",
    "        return 2  # Evening\n",
    "    else:\n",
    "        return 3  # Midnight\n",
    "\n",
    "# Create the 'Time_Period' column with numeric codes\n",
    "traffic_df['Time_Period'] = pd.to_datetime(traffic_df['START_DT']).dt.hour.apply(assign_time_period)\n",
    "\n",
    "# Verify the modified DataFrame\n",
    "print(\"\\nModified DataFrame with 'Time_Period':\")\n",
    "print(traffic_df[['START_DT', 'Time_Period']].head())\n",
    "\n",
    "# Save the modified DataFrame to a CSV file\n",
    "if debug_mode:\n",
    "    traffic_df.to_csv(\"justToCheck4.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Data cleaning logic based on the selected method\n",
    "if cleaning_method == 'manual':\n",
    "    print(\"Running manual data cleaning...\")\n",
    "    \n",
    "    # Define the start and end dates (shift start date backward by 14 days to include history)\n",
    "    start_date = traffic_df['Date'].min()  # Shift start date backward by 14 days to include history\n",
    "    end_date = traffic_df['Date'].max()\n",
    "    print(f\"Traffic Incidents Data Covers from {start_date} to {end_date}\")\n",
    "\n",
    "    # Generate all combinations of dates and time periods\n",
    "    all_dates = pd.date_range(start=start_date, end=end_date)\n",
    "    time_periods = [0, 1, 2, 3]  # Four time periods in a day\n",
    "\n",
    "    # Create a DataFrame for all combinations of dates and time periods\n",
    "    date_time_combinations = list(product(all_dates, time_periods))\n",
    "    xydata = pd.DataFrame(date_time_combinations, columns=['Date', 'Time_Period'])\n",
    "\n",
    "    # Add columns for clusters based on n_clusters value\n",
    "    for cluster_num in range(n_clusters):\n",
    "        xydata[f'Cluster{cluster_num}'] = 0  # Initialize with 0 value for every cluster\n",
    "\n",
    "    # Aggregate traffic_df to get counts of incidents per cluster\n",
    "    traffic_counts = traffic_df.groupby(['Date', 'Time_Period', 'QUADRANT']).size().reset_index(name='Count')\n",
    "    # Map for conversion\n",
    "    quadrant_mapping = {'NE': 0, 'NW': 1, 'SE': 2, 'SW': 3}\n",
    "    # Replace QUADRANT values with numeric codes\n",
    "    traffic_counts['QUADRANT'] = traffic_counts['QUADRANT'].map(quadrant_mapping)\n",
    "\n",
    "    # Update the counts in xydata DataFrame\n",
    "    for cluster_num in range(n_clusters):\n",
    "        # Extract the data for the current cluster\n",
    "        cluster_data = traffic_counts[traffic_counts['QUADRANT'] == cluster_num]\n",
    "        # Iterate through the rows and update the counts in xydata\n",
    "        for _, row in cluster_data.iterrows():\n",
    "            mask = (xydata['Date'] == row['Date']) & (xydata['Time_Period'] == row['Time_Period'])\n",
    "            xydata.loc[mask, f'Cluster{cluster_num}'] += int(row['Count'])\n",
    "\n",
    "    # Export the result to CSV\n",
    "    if debug_mode:\n",
    "        xydata.to_csv(\"justToCheck5.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running unsupervised clustering for data cleaning...\n",
      "Traffic Incidents Data Covers from 2016-12-06 00:00:00 to 2024-11-14 00:00:00\n"
     ]
    }
   ],
   "source": [
    "if cleaning_method == 'unsupervised':\n",
    "    print(\"Running unsupervised clustering for data cleaning...\")\n",
    "\n",
    "    # Define the start and end dates (shift start date backward by 14 days to include history)\n",
    "    start_date = traffic_df['Date'].min()  # Shift start date backward by 14 days to include history\n",
    "    end_date = traffic_df['Date'].max()\n",
    "    print(f\"Traffic Incidents Data Covers from {start_date} to {end_date}\")\n",
    "\n",
    "    # Generate all combinations of dates and time periods\n",
    "    all_dates = pd.date_range(start=start_date, end=end_date)\n",
    "    time_periods = [0, 1, 2, 3]  # Four time periods in a day\n",
    "\n",
    "    # Create a DataFrame for all combinations of dates and time periods\n",
    "    date_time_combinations = list(product(all_dates, time_periods))\n",
    "    xydata = pd.DataFrame(date_time_combinations, columns=['Date', 'Time_Period'])\n",
    "\n",
    "    # Determine the number of unique clusters\n",
    "    n_clusters = traffic_df['Cluster_KMeans'].max() + 1\n",
    "\n",
    "    # Add columns for clusters based on n_clusters value\n",
    "    for cluster_num in range(n_clusters):\n",
    "        xydata[f'Cluster{cluster_num}'] = 0  # Initialize with 0 value for every cluster\n",
    "\n",
    "    # Aggregate traffic_df to get counts of incidents per cluster\n",
    "    traffic_counts = traffic_df.groupby(['Date', 'Time_Period', 'Cluster_KMeans']).size().reset_index(name='Count')\n",
    "\n",
    "    # Update the counts in xydata DataFrame\n",
    "    for cluster_num in range(n_clusters):\n",
    "        # Extract the data for the current cluster\n",
    "        cluster_data = traffic_counts[traffic_counts['Cluster_KMeans'] == cluster_num]\n",
    "        # Iterate through the rows and update the counts in xydata\n",
    "        for _, row in cluster_data.iterrows():\n",
    "            mask = (xydata['Date'] == row['Date']) & (xydata['Time_Period'] == row['Time_Period'])\n",
    "            xydata.loc[mask, f'Cluster{cluster_num}'] += int(row['Count'])\n",
    "\n",
    "    # Export the result to CSV\n",
    "    if debug_mode:\n",
    "        xydata.to_csv(\"justToCheck5.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Date  Time_Period  Cluster0  Cluster1  Cluster2  Cluster3  Cluster4  \\\n",
      "0 2016-12-06            0         0         0         0         0         0   \n",
      "1 2016-12-06            1         0         1         0         1         0   \n",
      "2 2016-12-06            2         0         0         2         0         0   \n",
      "3 2016-12-06            3         0         0         0         0         0   \n",
      "4 2016-12-07            0         1         1         1         1         3   \n",
      "\n",
      "   Cluster5  C0D-1HA  C0D-2HA  ...  C3D-3HA  C3D-4HA  C4D-1HA  C4D-2HA  \\\n",
      "0         1        0        0  ...        0        0        0        0   \n",
      "1         4        0        0  ...        0        0        0        0   \n",
      "2         2        0        0  ...        0        0        0        0   \n",
      "3         0        0        0  ...        0        0        0        0   \n",
      "4         1        0        0  ...        0        0        0        0   \n",
      "\n",
      "   C4D-3HA  C4D-4HA  C5D-1HA  C5D-2HA  C5D-3HA  C5D-4HA  \n",
      "0        0        0        0        0        0        0  \n",
      "1        0        0        0        0        0        0  \n",
      "2        0        0        0        0        0        0  \n",
      "3        0        0        0        0        0        0  \n",
      "4        0        0        0        0        0        0  \n",
      "\n",
      "[5 rows x 32 columns]\n"
     ]
    }
   ],
   "source": [
    "# Generate new columns for each cluster for each history day\n",
    "for cluster_num in range(n_clusters):\n",
    "    for day in range(1, n_history_days + 1):\n",
    "        column_name = f'C{cluster_num}D-{day}HA'\n",
    "        xydata[column_name] = 0  # Initialize with 0 or another suitable value\n",
    "\n",
    "# Display the DataFrame to verify new columns\n",
    "print(xydata.head())\n",
    "\n",
    "# Export the result to CSV\n",
    "if debug_mode:\n",
    "    xydata.to_csv(\"justToCheck6.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Date  Time_Period  Cluster0  Cluster1  Cluster2  Cluster3  Cluster4  \\\n",
      "0 2016-12-06            0         0         0         0         0         0   \n",
      "1 2016-12-06            1         0         1         0         1         0   \n",
      "2 2016-12-06            2         0         0         2         0         0   \n",
      "3 2016-12-06            3         0         0         0         0         0   \n",
      "4 2016-12-07            0         1         1         1         1         3   \n",
      "\n",
      "   Cluster5  C0D-1HA  C0D-2HA  ...  C3D-3HA  C3D-4HA  C4D-1HA  C4D-2HA  \\\n",
      "0         1      NaN      NaN  ...      NaN      NaN      NaN      NaN   \n",
      "1         4      NaN      NaN  ...      NaN      NaN      NaN      NaN   \n",
      "2         2      NaN      NaN  ...      NaN      NaN      NaN      NaN   \n",
      "3         0      NaN      NaN  ...      NaN      NaN      NaN      NaN   \n",
      "4         1      0.0      NaN  ...      NaN      NaN      0.0      NaN   \n",
      "\n",
      "   C4D-3HA  C4D-4HA  C5D-1HA  C5D-2HA  C5D-3HA  C5D-4HA  \n",
      "0      NaN      NaN      NaN      NaN      NaN      NaN  \n",
      "1      NaN      NaN      NaN      NaN      NaN      NaN  \n",
      "2      NaN      NaN      NaN      NaN      NaN      NaN  \n",
      "3      NaN      NaN      NaN      NaN      NaN      NaN  \n",
      "4      NaN      NaN      1.0      NaN      NaN      NaN  \n",
      "\n",
      "[5 rows x 32 columns]\n"
     ]
    }
   ],
   "source": [
    "# Ensure the DataFrame is sorted by Date for correct shifting\n",
    "xydata.sort_values(by=['Date', 'Time_Period'], inplace=True)\n",
    "\n",
    "# Generate new columns for each cluster for each history day and fill with shifted data\n",
    "for cluster_num in range(n_clusters):\n",
    "    for day in range(1, n_history_days + 1):\n",
    "        column_name = f'C{cluster_num}D-{day}HA'\n",
    "        # Shift data within each time period group\n",
    "        xydata[column_name] = xydata.groupby('Time_Period')[f'Cluster{cluster_num}'].shift(day)\n",
    "\n",
    "# Display the DataFrame to verify new columns\n",
    "print(xydata.head())\n",
    "\n",
    "# Optional: Export the result to CSV\n",
    "if debug_mode:\n",
    "    xydata.to_csv(\"justToCheck7.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Cluster0', 'Cluster1', 'Cluster2', 'Cluster3', 'Cluster4', 'Cluster5']\n",
      "        Date  Time_Period  Cluster0  Cluster1  Cluster2  Cluster3  Cluster4  \\\n",
      "0 2016-12-06            0         0         0         0         0         0   \n",
      "1 2016-12-06            1         0         1         0         1         0   \n",
      "2 2016-12-06            2         0         0         1         0         0   \n",
      "3 2016-12-06            3         0         0         0         0         0   \n",
      "4 2016-12-07            0         1         1         1         1         1   \n",
      "\n",
      "   Cluster5  C0D-1HA  C0D-2HA  ...  C3D-4HA  C4D-1HA  C4D-2HA  C4D-3HA  \\\n",
      "0         1      NaN      NaN  ...      NaN      NaN      NaN      NaN   \n",
      "1         1      NaN      NaN  ...      NaN      NaN      NaN      NaN   \n",
      "2         1      NaN      NaN  ...      NaN      NaN      NaN      NaN   \n",
      "3         0      NaN      NaN  ...      NaN      NaN      NaN      NaN   \n",
      "4         1      0.0      NaN  ...      NaN      0.0      NaN      NaN   \n",
      "\n",
      "   C4D-4HA  C5D-1HA  C5D-2HA  C5D-3HA  C5D-4HA  Total_Accidents  \n",
      "0      NaN      NaN      NaN      NaN      NaN                1  \n",
      "1      NaN      NaN      NaN      NaN      NaN                6  \n",
      "2      NaN      NaN      NaN      NaN      NaN                4  \n",
      "3      NaN      NaN      NaN      NaN      NaN                0  \n",
      "4      NaN      1.0      NaN      NaN      NaN                8  \n",
      "\n",
      "[5 rows x 33 columns]\n"
     ]
    }
   ],
   "source": [
    "# List of cluster columns to transform\n",
    "cluster_columns = [f'Cluster{i}' for i in range(n_clusters)]  \n",
    "print(cluster_columns)\n",
    "# Calculate the total number of accidents for each row\n",
    "xydata['Total_Accidents'] = xydata[cluster_columns].sum(axis=1)\n",
    "\n",
    "# Apply transformation to each cluster column\n",
    "for column in cluster_columns:\n",
    "    xydata[column] = xydata[column].apply(lambda x: 1 if x > 0 else 0)\n",
    "\n",
    "# # Apply transformation to each cluster column\n",
    "# for column in cluster_columns:\n",
    "#     xydata[column] = xydata.apply(\n",
    "#         lambda row: (row[column] / row['Total_Accidents']) * 100 if row['Total_Accidents'] > 0 else 0,\n",
    "#         axis=1\n",
    "#     )\n",
    "\n",
    "# Display the DataFrame to verify the changes\n",
    "print(xydata.head())\n",
    "\n",
    "# Export the result to CSV\n",
    "xydata.to_csv(\"xydata_trafic.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
