{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traffic Incidents Data Covers from 2016-12-06 to 2024-11-14\n",
      "Wealther Incidents Data Covers from 2015-01-01 to 2024-12-31\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the accident and weather datasets\n",
    "accident_data = pd.read_csv(\"xydata_trafic.csv\")\n",
    "weather_data = pd.read_csv(\"xydata_weather.csv\")\n",
    "\n",
    "# Ensure the Date column is in datetime format\n",
    "accident_data[\"Date\"] = pd.to_datetime(accident_data[\"Date\"])\n",
    "weather_data[\"Date\"] = pd.to_datetime(weather_data[\"Date\"])\n",
    "\n",
    "# Verify that both datasets have 'Date' and 'Time Period'\n",
    "assert \"Date\" in accident_data.columns and \"Time_Period\" in accident_data.columns, \"Accident data is missing required columns.\"\n",
    "assert \"Date\" in weather_data.columns and \"Time_Period\" in weather_data.columns, \"Weather data is missing required columns.\"\n",
    "\n",
    "# # Determine the start and end START_DT of the filtered data\n",
    "accident_start_date = accident_data['Date'].min().date()\n",
    "accident_end_date = accident_data['Date'].max().date()\n",
    "weather_start_date = weather_data['Date'].min().date()\n",
    "weather_end_date = weather_data['Date'].max().date()\n",
    "print(f\"Traffic Incidents Data Covers from {accident_start_date} to {accident_end_date}\")\n",
    "print(f\"Wealther Incidents Data Covers from {weather_start_date} to {weather_end_date}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered accident data shape: (11444, 33)\n",
      "Filtered weather data shape: (11444, 12)\n"
     ]
    }
   ],
   "source": [
    "# Filter the data for the specified date range\n",
    "accident_data_filtered = accident_data[\n",
    "    (accident_data[\"Date\"] >= \"2017-01-01\") & (accident_data[\"Date\"] <= \"2024-10-31\")\n",
    "]\n",
    "weather_data_filtered = weather_data[\n",
    "    (weather_data[\"Date\"] >= \"2017-01-01\") & (weather_data[\"Date\"] <= \"2024-10-31\")\n",
    "]\n",
    "# Verify the filtered data\n",
    "print(f\"Filtered accident data shape: {accident_data_filtered.shape}\")\n",
    "print(f\"Filtered weather data shape: {weather_data_filtered.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged Data Covers from 2017-01-01 00:00:00 to 2024-10-31 00:00:00\n",
      "Total Rows in Merged Data: 11444\n",
      "        Date  Time_Period  Cluster0  Cluster1  Cluster2  Cluster3  Cluster4  \\\n",
      "0 2017-01-01            0         0         0         0         0         0   \n",
      "1 2017-01-01            1         0         0         0         0         0   \n",
      "2 2017-01-01            2         0         0         0         0         0   \n",
      "3 2017-01-01            3         0         0         0         0         0   \n",
      "4 2017-01-02            0         0         0         0         0         0   \n",
      "\n",
      "   Cluster5  C0D-1HA  C0D-2HA  ...  Temp (째C)  Dew Point Temp (째C)  \\\n",
      "0         0      0.0      0.0  ...   0.085667           -14.783333   \n",
      "1         0      0.0      0.0  ...   0.005667           -16.100000   \n",
      "2         0      0.0      0.0  ...   2.896000           -17.183333   \n",
      "3         0      0.0      0.0  ...   1.142667           -12.583333   \n",
      "4         0      0.0      0.0  ...   1.675000           -25.783333   \n",
      "\n",
      "   Rel Hum (%)  Wind Dir (10s deg)  Wind Spd (km/h)  Visibility (km)  \\\n",
      "0    79.833333           18.666667        25.500000         8.116667   \n",
      "1    70.833333           24.333333        16.666667        21.166667   \n",
      "2    79.000000           20.500000         9.500000        17.150000   \n",
      "3    83.666667            2.666667        23.833333         3.066667   \n",
      "4    78.000000           22.666667         2.500000        17.700000   \n",
      "\n",
      "   Stn Press (kPa)  Day_Of_Week  Is_Weekend  Is_Holiday  \n",
      "0        88.966667            6           1           1  \n",
      "1        89.108333            6           1           1  \n",
      "2        89.230000            6           1           1  \n",
      "3        88.545000            6           1           1  \n",
      "4        89.520000            0           0           0  \n",
      "\n",
      "[5 rows x 43 columns]\n",
      "Columns in dataset:\n",
      "['Date', 'Time_Period', 'Cluster0', 'Cluster1', 'Cluster2', 'Cluster3', 'Cluster4', 'Cluster5', 'C0D-1HA', 'C0D-2HA', 'C0D-3HA', 'C0D-4HA', 'C1D-1HA', 'C1D-2HA', 'C1D-3HA', 'C1D-4HA', 'C2D-1HA', 'C2D-2HA', 'C2D-3HA', 'C2D-4HA', 'C3D-1HA', 'C3D-2HA', 'C3D-3HA', 'C3D-4HA', 'C4D-1HA', 'C4D-2HA', 'C4D-3HA', 'C4D-4HA', 'C5D-1HA', 'C5D-2HA', 'C5D-3HA', 'C5D-4HA', 'Total_Accidents', 'Temp (째C)', 'Dew Point Temp (째C)', 'Rel Hum (%)', 'Wind Dir (10s deg)', 'Wind Spd (km/h)', 'Visibility (km)', 'Stn Press (kPa)', 'Day_Of_Week', 'Is_Weekend', 'Is_Holiday']\n"
     ]
    }
   ],
   "source": [
    "# Merge datasets on 'Date' and 'Time Period'\n",
    "merged_data = pd.merge(\n",
    "    accident_data_filtered,\n",
    "    weather_data_filtered,\n",
    "    on=[\"Date\", \"Time_Period\"],\n",
    "    how=\"inner\",\n",
    "    suffixes=('_accident', '_weather')\n",
    ")\n",
    "\n",
    "# Save the merged dataset to a new CSV file\n",
    "merged_data.to_csv(\"xydata_merged.csv\", index=False)\n",
    "\n",
    "# Display summary and first few rows\n",
    "print(f\"Merged Data Covers from {merged_data['Date'].min()} to {merged_data['Date'].max()}\")\n",
    "print(f\"Total Rows in Merged Data: {len(merged_data)}\")\n",
    "print(merged_data.head())\n",
    "print(\"Columns in dataset:\")\n",
    "print(merged_data.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the data for the specified date range\n",
    "filtered_data = merged_data[\n",
    "    (merged_data[\"Date\"] >= \"2017-12-01\") & (merged_data[\"Date\"] <= \"2018-03-31\")\n",
    "]\n",
    "# Save the merged dataset to a new CSV file\n",
    "filtered_data.to_csv(\"xydata_2017winter.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the data for the specified date range\n",
    "filtered_data = merged_data[\n",
    "    (merged_data[\"Date\"] >= \"2018-12-01\") & (merged_data[\"Date\"] <= \"2019-03-31\")\n",
    "]\n",
    "# Save the merged dataset to a new CSV file\n",
    "filtered_data.to_csv(\"xydata_2018winter.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the data for the specified date range\n",
    "filtered_data = merged_data[\n",
    "    (merged_data[\"Date\"] >= \"2019-12-01\") & (merged_data[\"Date\"] <= \"2020-03-31\")\n",
    "]\n",
    "# Save the merged dataset to a new CSV file\n",
    "filtered_data.to_csv(\"xydata_2019winter.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the data for the specified date range\n",
    "filtered_data = merged_data[\n",
    "    (merged_data[\"Date\"] >= \"2020-12-01\") & (merged_data[\"Date\"] <= \"2021-03-31\")\n",
    "]\n",
    "# Save the merged dataset to a new CSV file\n",
    "filtered_data.to_csv(\"xydata_2020winter.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the data for the specified date range\n",
    "filtered_data = merged_data[\n",
    "    (merged_data[\"Date\"] >= \"2021-12-01\") & (merged_data[\"Date\"] <= \"2022-03-31\")\n",
    "]\n",
    "# Save the merged dataset to a new CSV file\n",
    "filtered_data.to_csv(\"xydata_2021winter.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the data for the specified date range\n",
    "filtered_data = merged_data[\n",
    "    (merged_data[\"Date\"] >= \"2022-12-01\") & (merged_data[\"Date\"] <= \"2023-03-31\")\n",
    "]\n",
    "# Save the merged dataset to a new CSV file\n",
    "filtered_data.to_csv(\"xydata_2022winter.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the data for the specified date range\n",
    "filtered_data = merged_data[\n",
    "    (merged_data[\"Date\"] >= \"2023-12-01\") & (merged_data[\"Date\"] <= \"2024-03-31\")\n",
    "]\n",
    "# Save the merged dataset to a new CSV file\n",
    "filtered_data.to_csv(\"xydata_2023winter.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
